{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "75d643f3-555b-43fd-8107-63a8d5c165a8",
   "metadata": {},
   "source": [
    "# Google News scraping with the GNews package"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86eeff10-857c-4a80-a002-2c114d17bae2",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Environment set-up and getting the company sample from the sustainability reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf478d7c-9148-4226-a33b-100bc8d9deba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gnews import GNews\n",
    "from newspaper import Article\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "import re\n",
    "import requests\n",
    "import os\n",
    "import spacy\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65eeb2e4-a4e4-438a-9d3a-0f352110e7f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_reports = '..\\\\data\\\\reports'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "507ff15d-d68d-41b2-8123-89247b160278",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = os.listdir(path_reports)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c809e1f5-efb7-4221-b024-d1ddb2364da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting to panda series because I find it easier to manipulate\n",
    "company_list = [word.split('.')[0] for word in sample]\n",
    "company_list = pd.Series(company_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9633f630-918c-4649-9180-97138ffb1514",
   "metadata": {},
   "source": [
    "We're planning to use these company names in the Google News search engine. First, to do that though, we need to replace the ampersand character with '%26' so that the search engine can read it as an ampersand character - this only applies to P&G and H&M. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a751e3a-5cce-4f55-9fda-6bc7a830c957",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_list = pd.Series(company_list)\n",
    "search_list = search_list.str.replace('&','%26')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba1892b3-d968-4140-ab2e-3c5ca595d2ff",
   "metadata": {},
   "source": [
    "I also go ahead here and change the names of the companies here so that I can use it when excluding publisher names - see below. This is subject to change based on what kind of websites companies use to share news."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "320ec1ab-cf2b-4742-b7c7-880806446bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "publisher_list = company_list.str.replace('-',' ') #so make it two words\n",
    "publisher_list = publisher_list.str.replace('ford motor', 'ford')\n",
    "publisher_list = publisher_list.str.replace('p&g', 'procter')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c71c2630-84ee-4e3a-b078-aafcb3fc3053",
   "metadata": {},
   "source": [
    "## Creating a search loop on Google News"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61aa644e-093f-4a76-974b-7232bd573228",
   "metadata": {},
   "source": [
    "First, I define the GNews object - we search through English news, which are published after the 1st January 2021. This does have an effect of limiting results and for further research it might be useful to expand this to different languages and implement translation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a3bda04-f32e-429a-864d-640f19b42449",
   "metadata": {},
   "outputs": [],
   "source": [
    "gn = GNews(language = 'en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d297f75d-43d0-4002-927b-c6a7993eb26c",
   "metadata": {},
   "outputs": [],
   "source": [
    "##creating a function to filter publishers so as to not include the companies themselves\n",
    "def filter_publisher(publisher,df):\n",
    "    company_filter = []\n",
    "    for i in range(len(df)):\n",
    "        if (publisher in df['publisher_name'][i].lower())|(publisher in df['publisher_link'][i].lower()):\n",
    "            company_filter.append(False)\n",
    "        else:\n",
    "            company_filter.append(True)\n",
    "    return df[company_filter]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d7802c-74d9-4ab5-ae49-b8ac95a09de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # creating a function to filter columns based on keywords\n",
    "def filter_on_column(keywords,df,column):\n",
    "    # creating a regex pattern\n",
    "    pattern = '|'.join(list(set(keywords)))\n",
    "    # creating the masking filter \n",
    "    masking = []\n",
    "    for string in column:\n",
    "        if re.search(pattern,string.lower()):\n",
    "            masking.append(True)\n",
    "        else: \n",
    "            masking.append(False)\n",
    "    return df[masking]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b89a8fc5-47de-418e-8f3f-2557e95207ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # creating a function to filter columns based on keywords\n",
    "def filter_out_column(keywords,df,column):\n",
    "    # creating a regex pattern\n",
    "    pattern = '|'.join(list(set(keywords)))\n",
    "    # creating the masking filter \n",
    "    masking = []\n",
    "    for string in column:\n",
    "        if re.search(pattern,string.lower()):\n",
    "            masking.append(False)\n",
    "        else: \n",
    "            masking.append(True)\n",
    "    return df[masking]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d364be8a-5420-460f-9db0-99f58699bdd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df = pd.DataFrame()\n",
    "for idx,company in enumerate(search_list[:30]):\n",
    "    # generate search results - form of a dictionary\n",
    "    # run three different searches due to Google News having a limit of 100 articles and too many keywords cause the search function to act strangely\n",
    "    search_1 = gn.get_news(f'allintitle:{company} sustainability OR sustainable OR climate OR environment OR environmental OR pollution OR pollute OR emission OR solar OR recycle OR recycling after:2021-01-01')\n",
    "    search_2 = gn.get_news(f'allintitle:{company} emissions OR recycles OR recycled OR pollutes OR polluted OR polluting OR wind OR plastic OR deforestation OR greenhouse OR waste OR biodiversity OR renewable after:2021-01-01')\n",
    "    search_3 = gn.get_news(f'allintitle:{company} reusing OR reuse OR reused OR reuses OR reusable OR biodegradable OR circular OR CO2 OR ecology OR ecological OR ecosystem OR greenwash OR greenwashing after:2021-01-01')\n",
    "    search = search_1 + search_2 + search_3\n",
    "    \n",
    "    # create a temporary data frame from search results\n",
    "    temp_df = pd.DataFrame.from_dict(search)\n",
    "    \n",
    "    # get publisher link and name from the publisher column\n",
    "    temp_df['publisher_link'] = temp_df['publisher'].apply(lambda x: x['href'])\n",
    "    temp_df['publisher_name'] = temp_df['publisher'].apply(lambda x: x['title'])\n",
    "    temp_df.drop(['description','publisher'], axis = 1, inplace = True)\n",
    "    \n",
    "    # removing the publisher if the publisher is the company itself by creating a filter that checks for whether the company name is in the publisher name\n",
    "    temp_df = filter_publisher(publisher_list[idx],temp_df)\n",
    "    \n",
    "    # remove any duplicates in temp_df\n",
    "    temp_df.drop_duplicates(subset = ['title'], inplace = True)\n",
    "    \n",
    "    # add the company name as a column\n",
    "    temp_df['company'] = company_list[idx]\n",
    "    \n",
    "    # add the temporary df to our full df\n",
    "    full_df = pd.concat([full_df, temp_df])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b68679-752a-4340-bd64-05f3e5e68b5c",
   "metadata": {},
   "source": [
    "## Getting the full article texts and full article titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35b7e52e-9def-4072-ab43-fc55cfdfe797",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "text_lst = []\n",
    "title_lst = []\n",
    "for link in full_df['url'][2000:]:\n",
    "    try:\n",
    "        article = gn.get_full_article(link)\n",
    "        text_lst.append(article.text)\n",
    "        title_lst.append(article.title)\n",
    "    except:\n",
    "        text_lst.append('webscraping not possible')\n",
    "        title_lst.append('webscraping not possible')\n",
    "full_df['text'] = text_lst\n",
    "full_df['title_full'] = title_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c2a8136-9bb1-4e31-ace0-e389f49c5ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_complete = full_df.copy()\n",
    "df_complete = pd.read_csv('df_full.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f363c23-059f-4acd-9bc3-0f1ad9c52f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing the rows where the webscraping was not possible\n",
    "df_filtered = df_complete[df_complete['title_full']!='webscraping not possible']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58ae45df-d944-4f5d-b2bc-9f188def59d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # creating a function to filter for sustainability topics based on keywords\n",
    "# def filter_sustainability(keywords,df):\n",
    "#     # creating a regex pattern\n",
    "#     pattern = '|'.join(list(set(keywords)))\n",
    "#     # creating the masking filter \n",
    "#     masking = []\n",
    "#     for title in df['title_full']:\n",
    "#         if re.search(pattern,title.lower()):\n",
    "#             masking.append(True)\n",
    "#         else: \n",
    "#             masking.append(False)\n",
    "#     return df[masking]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a3c44b-78bb-4b2e-9ba2-e141941809c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining the sustainability keywords for topic detection in titles\n",
    "sust_keywords = ['biodiversity', 'climate', 'ecology', 'environment', 'emission', 'pollution', 'sustainable', 'CO2', 'deforestation', 'greenhouse', 'greenwash', 'COP2', 'pollutant', 'ecosystem', 'waste', 'sustain', 'sustainability', 'solar', 'recycle', 'wind', 'renewable', 'water', 'plastic', 'circular', 'biodegradable']\n",
    "stemmer = PorterStemmer()\n",
    "stemmed_sust = [stemmer.stem(word) for word in sust_keywords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bddbc508-5511-4736-8aba-50bef68493df",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered =  filter_on_column(stemmed_sust, df_filtered,df_filtered['title_full'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "359a9b5e-67d5-462d-9a6c-2199cafa8420",
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing empty text\n",
    "df_filtered = df_filtered[df_filtered['text'].astype(bool)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b0ff61-97e4-48c5-ba48-f52618813be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered.drop('title', axis = 1, inplace = True)\n",
    "df_filtered.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c69ed6b4-ad73-4cf5-abf1-4f4b22180836",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "525336b2-0e39-4d62-bb56-0d95c1e50ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered['company'].value_counts()[df_filtered['company'].value_counts()<30]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "804b5067-5864-4fbe-9c2b-8db5f1273511",
   "metadata": {},
   "source": [
    "Beiersdorf has very few articles, which is not ideal, but we can work with this for now. To expand the number, I can either include more keywords in the initial search or expand the time range."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2be918d-011d-4dfb-9228-cc7430529afd",
   "metadata": {},
   "source": [
    "# Data Cleaning and Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e93c94-f661-4476-9b3a-561944494fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "j = 0\n",
    "for i,row in df_filtered.iterrows():\n",
    "    text = row['text']\n",
    "    try:\n",
    "        sent_tokenize(text)\n",
    "    except:\n",
    "        print(f'error at row {i}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5828cbe7-1421-4195-99be-1cb8d0d7e019",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_links = ['https://global.chinadaily.com.cn/a/202205/30/WS6294181ba310fd2b29e5fad8.html','https://www.biobased-diesel.com/post/nasa-boeing-gather-data-to-aid-saf-adoption',\n",
    "            'https://www.bluebiz.com/en/sustainability/innovation-hub/news/boeing-teams-up-with-mit-scientists/#:~:text=Boeing%20is%20partnering%20with%20scientists,the%20carbon%20emissions%20from%20aviation.','https://www.flyingmag.com/boeing-purchases-2-million-gallons-of-sustainable-aviation-fuel/',\n",
    "            'https://www.upstreamonline.com/energy-transition/chevron-delta-and-google-collaborate-in-biojet-fuel-data-tracking-plan/2-1-1064719', 'https://techtalksummits.com/news/tech-news/cisco-hp-and-dell-chasing-the-huge-360-ecosystem-goal-with-complimentary-tactics',\n",
    "            'https://euneighbourseast.eu/news/latest-news/ray-of-hope-eu-announces-donation-of-5700-solar-panels-to-ukraine/',\n",
    "            'https://www.rttnews.com/3312069/nasa-google-team-up-to-help-local-governments-improve-tracking-air-pollution.aspx',\n",
    "            'https://weibold.com/pyrum-to-recycle-end-of-life-tires-from-mercedes-benz-vehicles-in-future',\n",
    "            'https://www.intelligentdatacentres.com/2022/11/11/airtrunk-and-clp-power-announce-innovative-renewable-energy-solution-in-hong-kong-for-microsoft/',\n",
    "            'https://www.eaglevoice.com/news/kirkwood-to-receive-over-300-solar-panels-in-donation/','https://finance.yahoo.com/news/shells-cracker-plant-pollution-prompts-154630697.html',\n",
    "            'https://lanxess.com/en/Media/Press-Releases/2023/01/LANXESS-and-TotalEnergies-to-cooperate-on-sustainable-styrene',\n",
    "            'https://www.freightcarbonzero.com/fcz-companies/volvo-trucks/volvo-lng-trucks-assists-arla-foods-in-reducing-carbon-emissions/543.supplierarticle',\n",
    "             'https://www.prnewswire.com/news-releases/volvo-trucks-showcases-new-zero-emissions-truck-301571323.html',\n",
    "             'https://thehill.com/policy/equilibrium-sustainability/3819867-walmart-stores-in-6-states-no-longer-provide-single-use-bags-at-checkout-which-states-are-next/'\n",
    "            ]\n",
    "#766,859,871,890,1055,1580,1773,2065,3132,3355,3652,4381,5011,5594,5603,5766\n",
    "#2093,2304,2459,2694,2892,3646,3659,4992,5130,5741 filled in manually\n",
    "to_drop = [62,234,286,446,696,904,918,932,1341,1565,1634,2440,2458,2514,2592,2640,2734,2829,3328,3411,3534,3617,3718,\n",
    "          3783,3875,4205,4330,4397,4946,5580,5732,5761]\n",
    "#606,62 require signup/buying subscription"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "233142b5-9d6c-4d99-84b2-9a9913bdf658",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean = df_filtered.drop(to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed55626a-3307-4274-bc2b-89f0e4a4b69a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "# using the new links to get the missing text\n",
    "j = 0\n",
    "for i,row in df_clean.iterrows():\n",
    "    text = row['text']\n",
    "    try:\n",
    "        sent_tokenize(text)\n",
    "    except:\n",
    "        article = gn.get_full_article(new_links[j])\n",
    "        row['text'] = article.text\n",
    "        j=j+1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64591c92-af7e-4217-bb1a-4e49d09ec2fa",
   "metadata": {},
   "source": [
    "I need to drop the duplicates in the entire dataframe and the publishers as well, instead of just running it on the temporary dataframe above. This is because certain companies may share certain articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58996ef9-c285-428d-8102-6379a8258bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean.drop_duplicates(subset = ['title_full'], inplace = True)\n",
    "df_clean.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87873980-92a2-4f98-8a60-4465121c3f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "checking = filter_on_column(publisher_list, df_clean, df_clean['publisher_name'])\n",
    "#found Microsoft and Walmart corporate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3556d8a-84f5-4699-92f1-4f7cafa52170",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean = df_clean[df_clean['publisher_name'] != 'Microsoft']\n",
    "df_clean = df_clean[df_clean['publisher_name'] != 'Walmart Corporate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5dfeb2d-a219-4f21-85e5-2be296820a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "118b44e2-2c47-4935-874f-9c20a56087ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_clean.to_csv('df_clean.csv',index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a267f1-92d1-45bf-8404-050d6671d847",
   "metadata": {},
   "source": [
    "## Removing duplicate articles based on title similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3940dbb8-e2d5-478f-bb9c-ef9e7d662cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sentence_transformers import SentenceTransformer, util, models\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20e086f7-7eab-413c-99cc-f131bc723dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer('sentence-transformers/paraphrase-MiniLM-L6-v2') # this model was trained for paraphrasing so it should work quite well for this task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a173693b-e600-42b1-a764-505c30b39a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to remove duplicate articles based on title similarity\n",
    "def remove_duplicates(df, threshold):\n",
    "    titles = df['title_full'].tolist()\n",
    "    embeddings = model.encode(titles)\n",
    "    indices = []\n",
    "    for i, emb1 in enumerate(embeddings):\n",
    "        if i in indices:\n",
    "            continue\n",
    "        for j, emb2 in enumerate(embeddings[i+1:]):\n",
    "            if get_similarity(emb1, emb2) > threshold:\n",
    "                indices.append(j+i+1)\n",
    "    return df.drop(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c32360-90d3-40bc-b28b-c2f0cb0682a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nodup = remove_duplicates(df_clean, 0.8)\n",
    "df_nodup.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bfbdec9-2c82-436e-baa8-f596accc8697",
   "metadata": {},
   "source": [
    "## Using Spacy to check for only named entities as companies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7eae82d-b87b-461d-9b01-bbda3322b45e",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_trf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffcb20ed-c955-4601-9bff-481a347a4c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking for companies, for which the name could be ambiguous\n",
    "checklist = ['apple','shell'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ada4c9-d95a-4fc7-9e4d-595fe0dda046",
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_filter = []\n",
    "for i,row in tqdm(df_nodup.iterrows(), total=df_nodup.shape[0]):\n",
    "    title = row['title_full']\n",
    "    company_raw = row['company']\n",
    "    if company_raw in checklist:\n",
    "        company = company_raw.replace('-',' ')\n",
    "        if company == 'mcdonald':\n",
    "            company = \"mcdonald's\"\n",
    "        else:\n",
    "            company = company \n",
    "        doc = nlp(title)\n",
    "        entity_list = [str(entity).lower() for entity in list(doc.ents)]\n",
    "        if company not in ' '.join(entity_list):\n",
    "            ner_filter.append(i)\n",
    "    else:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f852f502-3924-4ce8-8109-c190784867db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we find there is one missing value for the title full - removed \n",
    "df_nodup[df_nodup['title_full'].isnull()]\n",
    "df_nodup.dropna(subset = ['title_full'], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "151e4995-4119-4225-9aa5-f0ab05ae8f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nodup.drop(ner_filter, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3044943-f7f1-49e5-a669-c9da10f805dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nodup.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c9fdfb8-d8a7-4cfc-b5d2-1b22ee33e89a",
   "metadata": {},
   "source": [
    "## Filtering out company communication keywords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbf6b644-9cfb-4f5f-9b61-133c93d808e1",
   "metadata": {},
   "source": [
    "There are some articles, which just summarize the company's sustainability report, which we do not want as we want to separate the substantial from symbolic actions based on the report/news document type. We will filter out these keywords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "965e4e46-4f0f-41bf-b3bf-9108899e820b",
   "metadata": {},
   "outputs": [],
   "source": [
    "communication_keywords = ['annual report','progress report', 'sustainability report','impact report',\n",
    "                          'financial report','ESG performance report','environmental report',\n",
    "                         'head of sustainability', 'sustainability head', 'chief of sustainability']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da8a0c0f-045e-44d5-9d46-5ec58ea63a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nodup = filter_out_column(communication_keywords, df_nodup, df_nodup['title_full'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d6e1498-7a22-418c-b9c1-19b169d0ee3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nodup.reset_index(inplace = True, drop = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "956e82df-a3a1-441a-97ed-629a83806811",
   "metadata": {},
   "source": [
    "## Only keeping articles published from 2021 onwards and only keeping McDonald's or Ronald McDonald in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3647268d-a9c9-45e6-b3c5-6494aeaf037c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_clean = df_nodup.copy()\n",
    "df_clean = pd.read_csv('df_clean.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f12aa021-f8ee-4ca8-96ed-1f01d0a2ca9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean['published date'] = pd.to_datetime(df_clean['published date'])\n",
    "df_clean['published_year'] = df_clean['published date'].dt.year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5d6dc85-4b78-4bc8-870c-9072a3debb80",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean = df_clean[df_clean['published_year']>2020]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "674806fa-f97d-44c5-85f8-1495b343c1fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "non_mcdonalds = []\n",
    "for i,row in df_clean[df_clean['company'] == 'mcdonald'].iterrows():\n",
    "    title = row['title_full']\n",
    "    if (\"mcdonald's\" in title.lower())|(\"ronald mcdonald\" in title.lower())|(\"mcdonald’s\" in title.lower()):\n",
    "        continue\n",
    "    else:\n",
    "        non_mcdonalds.append(i)\n",
    "\n",
    "len(non_mcdonalds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bbd1ef5-4bac-4d80-8aaa-b277367a49a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean.drop(non_mcdonalds, inplace = True)\n",
    "df_clean.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "715b46c2-3065-469a-ac00-8de053b89fbc",
   "metadata": {},
   "source": [
    "## Visualizations included in the thesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "748b282a-04c2-4f73-839a-4a74cf744e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Group the data by year and count the number of articles\n",
    "df_by_year = df_clean.groupby('published_year')['text'].count()\n",
    "\n",
    "# Group the data by company and count the number of articles\n",
    "df_by_company = df_clean.groupby('company')['text'].count()\n",
    "\n",
    "# Create a bar chart of the number of articles per year\n",
    "plt.figure(figsize=(10,5))\n",
    "sns.barplot(x=df_by_year.index, y=df_by_year.values, color='skyblue')\n",
    "plt.title('Number of Articles per Year')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Number of Articles')\n",
    "plt.savefig('articles_per_year.png', dpi=300)\n",
    "plt.show()\n",
    "\n",
    "# Create a bar chart of the number of articles per company\n",
    "plt.figure(figsize=(10,5))\n",
    "sns.barplot(x=df_by_company.index, y=df_by_company.values, color='salmon')\n",
    "plt.title('Number of Articles per Company')\n",
    "plt.xlabel('Company')\n",
    "plt.ylabel('Number of Articles')\n",
    "plt.xticks(rotation=90) # Rotate x-axis labels by 90 degrees\n",
    "plt.savefig('articles_per_company.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d904e502-ed37-4b9c-b2cf-d72b2cd8bd65",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Creating and cleaning the article sentence dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68bfee3f-095e-4185-a684-4303658453ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new dataframe to store the sentences\n",
    "df_article = pd.DataFrame(columns=['doc_type','company','sentence'])\n",
    "\n",
    "# loop over each row in the original dataframe and split the text into sentences\n",
    "for i, row in tqdm(df_clean.iterrows(),total = df_clean.shape[0]):\n",
    "    company = row['company']\n",
    "    text = row['text']\n",
    "    \n",
    "    # use the sentence tokenizer to split the text into sentences\n",
    "    sent_lst = []\n",
    "    for token in sent_tokenize(text):\n",
    "        sentences = token.split('\\n\\n')\n",
    "        for sentence in sentences:\n",
    "            # dealing with new lines inside the text\n",
    "            r_sent = ' '.join(sentence.split())\n",
    "            sent_lst.append(r_sent)\n",
    "    \n",
    "    # append each sentence as a new row in the df_article dataframe\n",
    "    for sentence in sent_lst:\n",
    "        temp = pd.DataFrame(\n",
    "            {\n",
    "                'doc_type': ['news'],\n",
    "                'company': [company],\n",
    "                'sentence': [sentence]\n",
    "            }\n",
    "        )\n",
    "        df_article = pd.concat([df_article,temp],ignore_index = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa41b9bd-c27c-4b36-a729-7e10f0ff69e4",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Remove non-ASCII values and other things"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b954c231-8cb7-4e1a-938e-7825d37b9322",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove non-ASCII characters since BERT can't read those\n",
    "df_article['sentence'] = df_article['sentence'].str.replace('’',\"'\")\n",
    "df_article['sentence'].replace(r'[^\\x00-\\x7F]+','', regex=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a82f03-93f0-4730-9508-bdbecb0e885a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_article[\"sentence\"] = df_article[\"sentence\"].str.replace('\"','')\n",
    "df_article['sentence'] = df_article['sentence'].str.replace('Nestl ', 'Nestle ')\n",
    "df_article['sentence'] = df_article['sentence'].str.replace('Mondel z', 'Mondelez')\n",
    "df_article[\"sentence\"] = df_article[\"sentence\"].replace(r'http\\S+|\\[.\\]:?|www\\S+|\\w+/\\S+|\\w+-\\w+-\\S+|\\[|\\]','',regex = True).replace(r'^\\s+|\\s+$','',regex=True).replace(r'\\s{2,}',' ',regex=True)\n",
    "# original: http\\S+|\\[.\\]:?|www\\S+|\\w+/\\S+|\\w+-\\w+-\\S+\n",
    "#remove double spaces with one space and remove most hyperlinks + remove whitespaces at the end and beginning of a sentence\n",
    "df_article[\"word count\"] = [len(i) for i in df_article[\"sentence\"].str.split()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25a0d807-b400-4318-9070-9e8eba61402d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_article = df_article[df_article[\"word count\"] > 5]\n",
    "df_article = df_article[df_article[\"word count\"] < 100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee8109d-248f-40d0-92c6-2b7ed8d3a3a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping any duplicate sentences - surprisingly there are a lot of them\n",
    "df_article.drop_duplicates(subset = ['sentence'], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "240b5e22-0af3-4ae0-9351-3911a10152f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_article.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff63499-defd-4e63-b73a-860fa268717c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to check if a sentence is comprised of more than half uppercase characters\n",
    "def is_mostly_uppercase(sentence):\n",
    "    return sum(1 for c in sentence if c.isupper()) / len(sentence) > 0.5\n",
    "\n",
    "# apply the function to the 'sentence' column and filter out the rows where the condition is True\n",
    "df_article = df_article[~df_article['sentence'].apply(is_mostly_uppercase)]\n",
    "\n",
    "# print the resulting dataframe\n",
    "df_article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b89cb9-dcd6-47f2-801e-4c7ce3f8686b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_article.reset_index(inplace = True, drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8e63b04-256f-468a-83e0-5c137c94b5f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_article.to_csv(os.path.join(path_data, 'article_sentences_gnews.csv'), index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
