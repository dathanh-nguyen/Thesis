{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6855a741-ecb2-4f7f-8316-75bde5677651",
   "metadata": {},
   "source": [
    "# Set-up and data loading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acc4f19f-f161-4118-8f40-26f22af2e1a8",
   "metadata": {},
   "source": [
    "**Note**: Much of the code here is taken from https://github.com/llbtl/paper_ssm01/tree/main. Any copyright of the code belongs to the authors of that paper. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a792e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import fitz\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7df52c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_articles = '..\\\\data\\\\articles'\n",
    "fname_out = '..\\\\data_structured\\\\article_sentences_pdf.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9b36633a-df5e-4d84-8d17-435d6fb27270",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['beiersdorf.PDF',\n",
       " 'colgate.PDF',\n",
       " 'diageo.PDF',\n",
       " 'ford-motor.PDF',\n",
       " 'general-mills.PDF',\n",
       " 'henkel.PDF',\n",
       " 'hershey.PDF',\n",
       " 'inditex.PDF',\n",
       " 'komatsu.PDF',\n",
       " 'linde.PDF',\n",
       " 'mondelez.PDF',\n",
       " 'ralph-lauren.PDF',\n",
       " 'sonoco.PDF']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(path_articles)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45481d47-d0c1-4a09-af0b-c22c5d394f17",
   "metadata": {},
   "source": [
    "# Loading and Processing the NexisUni Articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3445efbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cnt(text):\n",
    "    cnt = 0\n",
    "    for word in text.split():\n",
    "        if word.isalnum():\n",
    "            cnt += 1\n",
    "    return cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a99207d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text(block_lst):\n",
    "\n",
    "    \n",
    "    text_lst = []\n",
    "    for block in block_lst:\n",
    "        if block[6] != 0: continue # block_type: 0 = text\n",
    "    \n",
    "        text = ''.join([i if ord(i) < 128 else ' ' for i in block[4]]) #removes non-ascii characters already \n",
    "    \n",
    "        #if get_cnt(text) > 5: \n",
    "        text_lst.append(text)\n",
    "        \n",
    "    return (text_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "79a96f84-2a61-4807-ba30-4e83a827426a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentence(fname):\n",
    " \n",
    "    doc = fitz.open(fname)\n",
    "    \n",
    "    text_lst = []\n",
    "    for page_no, page in enumerate(doc):\n",
    "        block_lst = page.get_text_blocks()\n",
    "        text = get_text(block_lst)\n",
    "        text_lst += text\n",
    "    \n",
    "    lst = []\n",
    "    for i,text in enumerate(text_lst):\n",
    "        # this is to take only the body of the text in the article pdfs\n",
    "        if text == \"Body\\n\":\n",
    "            beg = i+1\n",
    "        if text == \"End of Document\\n\":\n",
    "            end = i\n",
    "            chunk = text_lst[beg:end]\n",
    "            chunk[:] = (text for text in chunk if get_cnt(text) > 5)\n",
    "        # for block in chunk:\n",
    "        #     if get_cnt(block) < 5:\n",
    "        #         chunk.remove(block)\n",
    "        #     #block.replace('-\\n', '')\n",
    "            to_tokenize ='\\n'.join(chunk)\n",
    "            sent_lst = []\n",
    "            for token in sent_tokenize(to_tokenize):\n",
    "                sentences = token.split('\\n\\n')\n",
    "                for sentence in sentences:\n",
    "                    r_sent = ' '.join(sentence.split())\n",
    "                    sent_lst.append(r_sent)\n",
    "            lst += sent_lst\n",
    "            \n",
    "    doc.close()\n",
    "    return lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2c025765",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_document(fname, sent_lst):\n",
    "\n",
    "    res_df = pd.DataFrame(\n",
    "        {\n",
    "            'doc_type': \"news\",\n",
    "            'company': fname.split(\".\")[0],\n",
    "            'sentence': sent_lst\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    return res_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "23aa88f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_filelist(path):\n",
    "\n",
    "    # Create empty DataFrame\n",
    "    df = pd.DataFrame()\n",
    "    \n",
    "    # Read file list (directory)\n",
    "    for idx, fname in enumerate(os.listdir(path)):\n",
    "        p_fname = os.path.join(path, fname)\n",
    "        print('path + fname >>>', p_fname)\n",
    "        \n",
    "        if p_fname.split('.')[-1] != 'PDF': continue\n",
    "        print('fname >>>',fname)\n",
    "    \n",
    "#         doc_id = int(idx)\n",
    "        \n",
    "#         print(f'doc_id = [{doc_id}], fname = [{fname}]')\n",
    "#         print('')\n",
    "    \n",
    "        sent_lst = get_sentence(p_fname)\n",
    "        df_doc   = gen_document(fname, sent_lst)\n",
    "        \n",
    "        df = pd.concat([df,df_doc])\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "24b348bb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "path + fname >>> C:\\Users\\tnguyen10\\OneDrive - Deloitte (O365D)\\Documents\\GitHub\\Thesis\\data\\articles\\beiersdorf.PDF\n",
      "fname >>> beiersdorf.PDF\n",
      "path + fname >>> C:\\Users\\tnguyen10\\OneDrive - Deloitte (O365D)\\Documents\\GitHub\\Thesis\\data\\articles\\colgate.PDF\n",
      "fname >>> colgate.PDF\n",
      "path + fname >>> C:\\Users\\tnguyen10\\OneDrive - Deloitte (O365D)\\Documents\\GitHub\\Thesis\\data\\articles\\diageo.PDF\n",
      "fname >>> diageo.PDF\n",
      "path + fname >>> C:\\Users\\tnguyen10\\OneDrive - Deloitte (O365D)\\Documents\\GitHub\\Thesis\\data\\articles\\ford-motor.PDF\n",
      "fname >>> ford-motor.PDF\n",
      "path + fname >>> C:\\Users\\tnguyen10\\OneDrive - Deloitte (O365D)\\Documents\\GitHub\\Thesis\\data\\articles\\general-mills.PDF\n",
      "fname >>> general-mills.PDF\n",
      "path + fname >>> C:\\Users\\tnguyen10\\OneDrive - Deloitte (O365D)\\Documents\\GitHub\\Thesis\\data\\articles\\henkel.PDF\n",
      "fname >>> henkel.PDF\n",
      "path + fname >>> C:\\Users\\tnguyen10\\OneDrive - Deloitte (O365D)\\Documents\\GitHub\\Thesis\\data\\articles\\hershey.PDF\n",
      "fname >>> hershey.PDF\n",
      "path + fname >>> C:\\Users\\tnguyen10\\OneDrive - Deloitte (O365D)\\Documents\\GitHub\\Thesis\\data\\articles\\inditex.PDF\n",
      "fname >>> inditex.PDF\n",
      "path + fname >>> C:\\Users\\tnguyen10\\OneDrive - Deloitte (O365D)\\Documents\\GitHub\\Thesis\\data\\articles\\komatsu.PDF\n",
      "fname >>> komatsu.PDF\n",
      "path + fname >>> C:\\Users\\tnguyen10\\OneDrive - Deloitte (O365D)\\Documents\\GitHub\\Thesis\\data\\articles\\linde.PDF\n",
      "fname >>> linde.PDF\n",
      "path + fname >>> C:\\Users\\tnguyen10\\OneDrive - Deloitte (O365D)\\Documents\\GitHub\\Thesis\\data\\articles\\mondelez.PDF\n",
      "fname >>> mondelez.PDF\n",
      "path + fname >>> C:\\Users\\tnguyen10\\OneDrive - Deloitte (O365D)\\Documents\\GitHub\\Thesis\\data\\articles\\ralph-lauren.PDF\n",
      "fname >>> ralph-lauren.PDF\n",
      "path + fname >>> C:\\Users\\tnguyen10\\OneDrive - Deloitte (O365D)\\Documents\\GitHub\\Thesis\\data\\articles\\sonoco.PDF\n",
      "fname >>> sonoco.PDF\n",
      "==== End of jobs ====\n",
      "CPU times: total: 1.73 s\n",
      "Wall time: 1.79 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df = read_filelist(path_articles)\n",
    "print('==== End of jobs ====')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ed2beaf-8acf-44e7-a373-23b058c29611",
   "metadata": {},
   "source": [
    "# Further Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f31a58f9-ffc8-4f3f-a7fb-98220a1fbdc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "143b1b92-b5a2-4f43-b949-7dff41b7d1af",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_article = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8a3be101-13a0-4660-abd9-71cfd0a8d0b8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# remove any quotes and unusual characters\n",
    "df_article[\"sentence\"] = df_article[\"sentence\"].str.replace('\"','', regex = True)\n",
    "df_article[\"sentence\"] = df_article[\"sentence\"].replace(r'http\\S+|\\[.\\]:?|www\\S+|\\w+/\\S+|\\w+-\\w+-\\S+|\\[|\\]','',regex = True).replace(r'^\\s+|\\s+$','',regex=True).replace(r'\\s{2,}',' ',regex=True)\n",
    "df_article['sentence'] = df_article['sentence'].str.replace('Nestl ', 'Nestle ')\n",
    "df_article['sentence'] = df_article['sentence'].str.replace('Mondel z', 'Mondelez')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d8e420d4-3df1-4f15-820e-6a828a36c041",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_article = df_article[df_article['sentence'].astype(bool)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3924b2db-0e71-48d7-8cfc-d45f198f952d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_article.reset_index(inplace = True, drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6f3c0fe0-16fa-459b-8ad3-0552850c3bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# my pdf package does not know how to deal with sentences that span across pages - define a funciton here, which will merge the two sentences following each other\n",
    "# if the previous one doesn't end with punctuation and the following starts with a lower case letter\n",
    "import string\n",
    "\n",
    "# define a function to check if a sentence ends with punctuation\n",
    "def ends_with_punctuation(s):\n",
    "    return s.strip()[-1] in string.punctuation\n",
    "\n",
    "# loop over each row in the DataFrame and concatenate the sentences as needed\n",
    "for i, row in df_article.iterrows():\n",
    "    # skip the first row as there is no previous row to compare with\n",
    "    if i == 0:\n",
    "        continue\n",
    "    \n",
    "    # get the current and previous sentences\n",
    "    prev_sentence = df_article.loc[i-1, 'sentence']\n",
    "    curr_sentence = df_article.loc[i, 'sentence']\n",
    "    \n",
    "    # check if the previous sentence ends with punctuation and the current sentence starts with a lowercase letter\n",
    "    if not ends_with_punctuation(prev_sentence) and curr_sentence[0].islower():\n",
    "        # concatenate the sentences with a space\n",
    "        df_article.at[i, 'sentence'] = prev_sentence + ' ' + curr_sentence\n",
    "        # drop the previous row\n",
    "        df_article.drop(i-1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fc95321e-f6de-4d72-8053-2a8da3a5fea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filtering based on word count\n",
    "df_article[\"word count\"] = [len(i) for i in df_article[\"sentence\"].str.split()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f38a85cf-0568-4387-a8d9-ad9ef44548db",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_article = df_article[df_article[\"word count\"] > 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7fb68b93-0a78-4af9-a2b4-45d546fb906f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_article = df_article[df_article[\"word count\"] < 100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "53391acf-4e93-4488-997d-7b53eb5b552c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_type</th>\n",
       "      <th>company</th>\n",
       "      <th>sentence</th>\n",
       "      <th>word count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>news</td>\n",
       "      <td>beiersdorf</td>\n",
       "      <td>This offers businesses in food and beverage, p...</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>news</td>\n",
       "      <td>beiersdorf</td>\n",
       "      <td>Designed with sustainability in mind, the tesa...</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>news</td>\n",
       "      <td>beiersdorf</td>\n",
       "      <td>To reduce the consumption of virgin plastic, u...</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>news</td>\n",
       "      <td>beiersdorf</td>\n",
       "      <td>70 percent of the polyethylene (PET) that make...</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>news</td>\n",
       "      <td>beiersdorf</td>\n",
       "      <td>The tape supports the circular economy and can...</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3944</th>\n",
       "      <td>news</td>\n",
       "      <td>sonoco</td>\n",
       "      <td>Newstex Authoritative Content is not read and ...</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3945</th>\n",
       "      <td>news</td>\n",
       "      <td>sonoco</td>\n",
       "      <td>Accordingly, neither Newstex nor its re-distri...</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3946</th>\n",
       "      <td>news</td>\n",
       "      <td>sonoco</td>\n",
       "      <td>The Newstex Authoritative Content shall be con...</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3947</th>\n",
       "      <td>news</td>\n",
       "      <td>sonoco</td>\n",
       "      <td>Accordingly, no warranties or other guarantees...</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3948</th>\n",
       "      <td>news</td>\n",
       "      <td>sonoco</td>\n",
       "      <td>Newstex and its re-distributors expressly rese...</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3807 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     doc_type     company                                           sentence  \\\n",
       "0        news  beiersdorf  This offers businesses in food and beverage, p...   \n",
       "1        news  beiersdorf  Designed with sustainability in mind, the tesa...   \n",
       "2        news  beiersdorf  To reduce the consumption of virgin plastic, u...   \n",
       "3        news  beiersdorf  70 percent of the polyethylene (PET) that make...   \n",
       "4        news  beiersdorf  The tape supports the circular economy and can...   \n",
       "...       ...         ...                                                ...   \n",
       "3944     news      sonoco  Newstex Authoritative Content is not read and ...   \n",
       "3945     news      sonoco  Accordingly, neither Newstex nor its re-distri...   \n",
       "3946     news      sonoco  The Newstex Authoritative Content shall be con...   \n",
       "3947     news      sonoco  Accordingly, no warranties or other guarantees...   \n",
       "3948     news      sonoco  Newstex and its re-distributors expressly rese...   \n",
       "\n",
       "      word count  \n",
       "0             15  \n",
       "1             23  \n",
       "2             24  \n",
       "3             16  \n",
       "4             19  \n",
       "...          ...  \n",
       "3944          12  \n",
       "3945          40  \n",
       "3946          12  \n",
       "3947          25  \n",
       "3948          17  \n",
       "\n",
       "[3807 rows x 4 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define a function to check if a sentence is comprised of more than half uppercase characters\n",
    "def is_mostly_uppercase(sentence):\n",
    "    return sum(1 for c in sentence if c.isupper()) / len(sentence) > 0.5\n",
    "\n",
    "# apply the function to the 'sentence' column and filter out the rows where the condition is True\n",
    "df_article = df_article[~df_article['sentence'].apply(is_mostly_uppercase)]\n",
    "\n",
    "# print the resulting dataframe\n",
    "df_article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b9180634-7c88-4140-a564-077c9e859978",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_article.reset_index(inplace = True, drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "28074afc-21cf-4818-b0a1-722afca01376",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_article.drop_duplicates(subset = ['sentence'], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "293f43f4-d59c-4dc2-9dcb-06232cdc53f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_article.to_csv(fname_out, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "278a5e2f-7a0f-4557-b4d0-502b82fca0aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.13.1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7b0bc10f-1e14-4a2b-896b-bbef57d3373e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at climatebert/distilroberta-base-climate-f were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at climatebert/distilroberta-base-climate-f and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline, AutoTokenizer, AutoModel, AutoModelForMaskedLM, AutoModelForSequenceClassification\n",
    "from scipy.spatial.distance import cosine\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "model_name = 'climatebert/distilroberta-base-climate-f'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "model = AutoModel.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "54986175-80d8-429a-9203-5a907aab46c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at climatebert/distilroberta-base-climate-f were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at climatebert/distilroberta-base-climate-f and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at climatebert/distilroberta-base-climate-f were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at climatebert/distilroberta-base-climate-f and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.998254427493864\n"
     ]
    }
   ],
   "source": [
    "z = [\"This product is environmentally friendly.\",\n",
    "    'The offering happens to be good for nature.',\n",
    "    'The product is unsustainable and destroys the environment.']\n",
    "\n",
    "def transformer_embedding (name, inp): \n",
    "    model = AutoModel.from_pretrained(name)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(name)\n",
    "    pipe = pipeline('feature-extraction', model=model, tokenizer = tokenizer)\n",
    "    features = pipe(inp)\n",
    "    features = np.squeeze(features)\n",
    "    return features\n",
    "\n",
    "embedding_features1=transformer_embedding(model_name,z[0])\n",
    "embedding_features2=transformer_embedding(model_name,z[2])\n",
    "distance=1-cosine(embedding_features1[0],embedding_features2[0])\n",
    "print(distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "753b91de-f669-44ee-9c72-e078b5d32ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences1 = df_article['sentence'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "dab3b747-ad67-4f99-98cc-4c6e3193865e",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(sentences1, return_tensors='pt', padding=True, truncation=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fbc5ea67-f001-44e8-95ce-feecb85f3120",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "31af7410-ffe6-4449-8ce9-1690b556763f",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'MaskedLMOutput' object has no attribute 'last_hidden_state'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m \u001b[43moutputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlast_hidden_state\u001b[49m\u001b[38;5;241m.\u001b[39mmean(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'MaskedLMOutput' object has no attribute 'last_hidden_state'"
     ]
    }
   ],
   "source": [
    "embeddings = outputs.last_hidden_state.mean(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "752e3de2-a588-44d2-b75f-4dc4d8ba6bae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.19340062, 0.5186796 , 0.40626413]], dtype=float32)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the sentences to embed\n",
    "sentences = [\"This product is environmentally friendly.\", \n",
    "             \"Jello is amazing food.\", \n",
    "             \"Hello darkness, my old friend.\"]\n",
    "\n",
    "sentences2 = ['Hello darkness, my old friend.',\n",
    "              'The company is greenwashing.',\n",
    "              'The company has increased its emissions by a large amount.']\n",
    "\n",
    "# Tokenize the sentences\n",
    "tokenized = tokenizer(sentences, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "# Generate the embeddings\n",
    "with torch.no_grad():\n",
    "    outputs = model(**tokenized)\n",
    "    embeddings = outputs.last_hidden_state.mean(dim=1)\n",
    "    \n",
    "# Tokenize the sentences\n",
    "tokenized2 = tokenizer(sentences2, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "# Generate the embeddings\n",
    "with torch.no_grad():\n",
    "    outputs2 = model(**tokenized2)\n",
    "    embeddings2 = outputs2.last_hidden_state.mean(dim=1)\n",
    "\n",
    "# Calculate the cosine similarity between all pairs of embeddings\n",
    "#cos_sim = torch.nn.functional.cosine_similarity(embeddings[0], embeddings2[0])\n",
    "\n",
    "# Convert the similarity matrix to a pandas dataframe\n",
    "#similarity_df = pd.DataFrame(similarity_matrix.numpy(), columns=sentences, index=sentences2)\n",
    "\n",
    "# Print the similarity matrix\n",
    "embeddings = embeddings.detach().numpy()\n",
    "embeddings2 = embeddings2.detach().numpy()\n",
    "\n",
    "cosine_similarity([embeddings[0]],\n",
    "                  embeddings2[0:]\n",
    "                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ec0b0713-9792-4f37-849d-bd1d5820fe73",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "d8e21c51-f30f-4a46-b2d8-16de3cf69b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = {'input_ids': [], 'attention_mask': []}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "c5fcff12-6b0d-4a2e-a2fc-002541b1303c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for sentence in sentences:\n",
    "    new_tokens = tokenizer.encode_plus(sentence, max_length = 512,\n",
    "                          truncation = True, padding = 'max_length',\n",
    "                          return_tensors = 'pt')\n",
    "    tokens['input_ids'].append(new_tokens['input_ids'][0]) #list within a list - we want to extrac the list with the 0\n",
    "    tokens['attention_mask'].append(new_tokens['attention_mask'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "ae4ba9a7-f232-4cb0-8f3f-dbc44c4db37f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens['input_ids'] = torch.stack(tokens['input_ids'])\n",
    "tokens['attention_mask'] = torch.stack(tokens['attention_mask'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "023a9846-c259-4211-a95a-b84a1138e934",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 512])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens['input_ids'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "696433a4-b6c6-4aa0-badc-d2ddad19fdda",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model(**tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "9011aa3b-c744-4288-8cf6-04a1be7d31f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['last_hidden_state', 'pooler_output'])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "14c54ce5-84c8-4007-b6a5-ee83f6c46939",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 512, 768])"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings = outputs.last_hidden_state\n",
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "0f3c67d1-aa21-46a1-9eb8-c8131f671cb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 512])"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention = tokens['attention_mask']\n",
    "attention.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "064ab4c9-5ea3-4e18-9295-352dae55eaaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = attention.unsqueeze(-1).expand(embeddings.shape).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "c2831b65-194b-4685-a6fb-b6b88636fce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_embeddings = embeddings*mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "b772926f-69e0-4c8d-bb38-340cf5646b95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 768])"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summed = torch.sum(mask_embeddings, 1)\n",
    "summed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "8b63f77c-2c8e-4e10-b850-de081204e7ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 768])"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts = torch.clamp(mask.sum(1), min = 1e-9)\n",
    "counts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "b16c13af-bf5b-4b1c-8483-803a2e67bbc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 768])"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_pooled = summed/counts\n",
    "mean_pooled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "9a319f44-635e-49d3-814b-bd7c6c4f2479",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.4212029 , 0.19536817]], dtype=float32)"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_pool = mean_pooled.detach().numpy()\n",
    "\n",
    "cosine_similarity([mean_pool[0]],\n",
    "                  mean_pool[1:]\n",
    "                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "71bcf19d-6550-4d71-9a25-c45106caebc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at climatebert/distilroberta-base-climate-f were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at climatebert/distilroberta-base-climate-f and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity score between sentences 1 and 2: 0.0029\n",
      "Similarity score between sentences 2 and 3: 0.0023\n",
      "Similarity score between sentences 3 and 4: 0.0029\n"
     ]
    }
   ],
   "source": [
    "# Load ClimateBERT tokenizer and model\n",
    "model_name = \"climatebert/distilroberta-base-climate-f\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "# Define input sentences\n",
    "sentences = [\n",
    "    \"This product is environmentally friendly.\",\n",
    "    'The offering happens to be good for nature.',\n",
    "    'The product is unsustainable and destroys the environment.']\n",
    "# Tokenize and encode the sentences\n",
    "encoded_sentences = [tokenizer.encode(s, add_special_tokens=True) for s in sentences]\n",
    "max_len = max(len(s) for s in encoded_sentences)\n",
    "padded_sentences = [s + [tokenizer.pad_token_id] * (max_len - len(s)) for s in encoded_sentences]\n",
    "inputs = torch.tensor(padded_sentences)\n",
    "\n",
    "# Obtain sentence embeddings\n",
    "with torch.no_grad():\n",
    "    outputs = model(inputs)\n",
    "    embeddings = outputs.last_hidden_state[:, 0, :]\n",
    "    \n",
    "# Normalize the embeddings\n",
    "norm_embeddings = normalize(embeddings.numpy())\n",
    "\n",
    "# Compute cosine similarity between sentence embeddings\n",
    "similarity_matrix = 1 - cosine_similarity(norm_embeddings)\n",
    "\n",
    "# Extract the upper triangular part of the matrix\n",
    "similarity_scores = similarity_matrix[np.triu_indices(len(sentences), k=1)]\n",
    "\n",
    "# Print the similarity scores\n",
    "for i, score in enumerate(similarity_scores):\n",
    "    print(f\"Similarity score between sentences {i+1} and {i+2}: {score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "60dd5325-9333-4ec9-83c7-1c8c14fbc2ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0214d64bf0f241388a3b92d52e8a65d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/480 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tnguyen10\\Anaconda3\\envs\\thesis\\lib\\site-packages\\huggingface_hub-0.13.1-py3.8.egg\\huggingface_hub\\file_download.py:133: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\tnguyen10\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0007f546453d41e3b1858aa5f89b76e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/331M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilroberta-base were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a66580c2325544e5be50871ca7768d32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)olve/main/vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aba51e4388ed42f0baed108d877dd141",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)olve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12cf99d5b2e4467ab32f9abfc571fefc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "feature_extraction = pipeline('feature-extraction', model=\"distilroberta-base\", tokenizer=\"distilroberta-base\")\n",
    "features = feature_extraction([\"This product is environmentally friendly.\",\n",
    "                               'The offering happens to be good for nature.',\n",
    "                               'The product is unsustainable and destroys the environment.'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "1aed2098-282f-4a00-aea3-125572be2cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent1=np.array(features[0][0][0])\n",
    "sent2=np.array(features[1][0][0])\n",
    "sent3=np.array(features[2][0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "bebcf9e4-bc2e-4bf7-b2de-959f2a116821",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent1=sent1.reshape(1,-1)\n",
    "sent2=sent2.reshape(1,-1)\n",
    "sent3 = sent3.reshape(1,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "650b62c9-4c47-4cc8-acea-05b312bc835a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.99916961]])"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(sent2,sent3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4927f1c4-3fce-4af5-9cd0-e94313446fee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
