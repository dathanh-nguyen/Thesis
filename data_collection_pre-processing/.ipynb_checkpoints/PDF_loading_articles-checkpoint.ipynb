{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6855a741-ecb2-4f7f-8316-75bde5677651",
   "metadata": {},
   "source": [
    "# Set-up and data loading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acc4f19f-f161-4118-8f40-26f22af2e1a8",
   "metadata": {},
   "source": [
    "**Note**: Much of the code related to loading and processing the articles here is taken from https://github.com/llbtl/paper_ssm01/tree/main. Any copyright of the code belongs to the authors of that paper. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a792e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import fitz\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7df52c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_articles = '..\\\\data\\\\articles'\n",
    "fname_out = '..\\\\data_structured\\\\article_sentences_pdf.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9b36633a-df5e-4d84-8d17-435d6fb27270",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['beiersdorf.PDF',\n",
       " 'colgate.PDF',\n",
       " 'diageo.PDF',\n",
       " 'ford-motor.PDF',\n",
       " 'general-mills.PDF',\n",
       " 'henkel.PDF',\n",
       " 'hershey.PDF',\n",
       " 'inditex.PDF',\n",
       " 'komatsu.PDF',\n",
       " 'linde.PDF',\n",
       " 'mondelez.PDF',\n",
       " 'ralph-lauren.PDF',\n",
       " 'sonoco.PDF']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(path_articles)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45481d47-d0c1-4a09-af0b-c22c5d394f17",
   "metadata": {},
   "source": [
    "# Loading and Processing the NexisUni Articles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8bd601e-7c99-41e5-8c50-b21e7496932d",
   "metadata": {},
   "source": [
    "The following functions help us process the PDF files (NexisUni articles) into a table, where each row is represented by a sentence from the article text. I split the document into sentences already as I will generate sentence embeddings later on using S-BERT. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3445efbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cnt(text):\n",
    "    cnt = 0\n",
    "    for word in text.split():\n",
    "        if word.isalnum():\n",
    "            cnt += 1\n",
    "    return cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a99207d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text(block_lst):\n",
    "    text_lst = []\n",
    "    for block in block_lst:\n",
    "        if block[6] != 0: continue # block_type: 0 = text\n",
    "    \n",
    "        text = ''.join([i if ord(i) < 128 else ' ' for i in block[4]]) #removes non-ascii characters already \n",
    "    \n",
    "        #if get_cnt(text) > 5: \n",
    "        text_lst.append(text)\n",
    "        \n",
    "    return (text_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "79a96f84-2a61-4807-ba30-4e83a827426a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentence(fname):\n",
    " \n",
    "    doc = fitz.open(fname)\n",
    "    \n",
    "    text_lst = []\n",
    "    for page_no, page in enumerate(doc):\n",
    "        block_lst = page.get_text_blocks()\n",
    "        text = get_text(block_lst)\n",
    "        text_lst += text\n",
    "    \n",
    "    lst = []\n",
    "    for i,text in enumerate(text_lst):\n",
    "        # this is to take only the body of the text in the article pdfs\n",
    "        if text == \"Body\\n\":\n",
    "            beg = i+1\n",
    "        if text == \"End of Document\\n\":\n",
    "            end = i\n",
    "            chunk = text_lst[beg:end]\n",
    "            chunk[:] = (text for text in chunk if get_cnt(text) > 5)\n",
    "        # for block in chunk:\n",
    "        #     if get_cnt(block) < 5:\n",
    "        #         chunk.remove(block)\n",
    "        #     #block.replace('-\\n', '')\n",
    "            to_tokenize ='\\n'.join(chunk)\n",
    "            sent_lst = []\n",
    "            for token in sent_tokenize(to_tokenize):\n",
    "                sentences = token.split('\\n\\n')\n",
    "                for sentence in sentences:\n",
    "                    r_sent = ' '.join(sentence.split())\n",
    "                    sent_lst.append(r_sent)\n",
    "            lst += sent_lst\n",
    "            \n",
    "    doc.close()\n",
    "    return lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2c025765",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_document(fname, sent_lst):\n",
    "\n",
    "    res_df = pd.DataFrame(\n",
    "        {\n",
    "            'doc_type': \"news\",\n",
    "            'company': fname.split(\".\")[0],\n",
    "            'sentence': sent_lst\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    return res_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "23aa88f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_filelist(path):\n",
    "\n",
    "    # Create empty DataFrame\n",
    "    df = pd.DataFrame()\n",
    "    \n",
    "    # Read file list (directory)\n",
    "    for idx, fname in enumerate(os.listdir(path)):\n",
    "        p_fname = os.path.join(path, fname)\n",
    "        print('path + fname >>>', p_fname)\n",
    "        \n",
    "        if p_fname.split('.')[-1] != 'PDF': continue\n",
    "        print('fname >>>',fname)\n",
    "    \n",
    "#         doc_id = int(idx)\n",
    "        \n",
    "#         print(f'doc_id = [{doc_id}], fname = [{fname}]')\n",
    "#         print('')\n",
    "    \n",
    "        sent_lst = get_sentence(p_fname)\n",
    "        df_doc   = gen_document(fname, sent_lst)\n",
    "        \n",
    "        df = pd.concat([df,df_doc])\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "24b348bb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "path + fname >>> C:\\Users\\tnguyen10\\OneDrive - Deloitte (O365D)\\Documents\\GitHub\\Thesis\\data\\articles\\beiersdorf.PDF\n",
      "fname >>> beiersdorf.PDF\n",
      "path + fname >>> C:\\Users\\tnguyen10\\OneDrive - Deloitte (O365D)\\Documents\\GitHub\\Thesis\\data\\articles\\colgate.PDF\n",
      "fname >>> colgate.PDF\n",
      "path + fname >>> C:\\Users\\tnguyen10\\OneDrive - Deloitte (O365D)\\Documents\\GitHub\\Thesis\\data\\articles\\diageo.PDF\n",
      "fname >>> diageo.PDF\n",
      "path + fname >>> C:\\Users\\tnguyen10\\OneDrive - Deloitte (O365D)\\Documents\\GitHub\\Thesis\\data\\articles\\ford-motor.PDF\n",
      "fname >>> ford-motor.PDF\n",
      "path + fname >>> C:\\Users\\tnguyen10\\OneDrive - Deloitte (O365D)\\Documents\\GitHub\\Thesis\\data\\articles\\general-mills.PDF\n",
      "fname >>> general-mills.PDF\n",
      "path + fname >>> C:\\Users\\tnguyen10\\OneDrive - Deloitte (O365D)\\Documents\\GitHub\\Thesis\\data\\articles\\henkel.PDF\n",
      "fname >>> henkel.PDF\n",
      "path + fname >>> C:\\Users\\tnguyen10\\OneDrive - Deloitte (O365D)\\Documents\\GitHub\\Thesis\\data\\articles\\hershey.PDF\n",
      "fname >>> hershey.PDF\n",
      "path + fname >>> C:\\Users\\tnguyen10\\OneDrive - Deloitte (O365D)\\Documents\\GitHub\\Thesis\\data\\articles\\inditex.PDF\n",
      "fname >>> inditex.PDF\n",
      "path + fname >>> C:\\Users\\tnguyen10\\OneDrive - Deloitte (O365D)\\Documents\\GitHub\\Thesis\\data\\articles\\komatsu.PDF\n",
      "fname >>> komatsu.PDF\n",
      "path + fname >>> C:\\Users\\tnguyen10\\OneDrive - Deloitte (O365D)\\Documents\\GitHub\\Thesis\\data\\articles\\linde.PDF\n",
      "fname >>> linde.PDF\n",
      "path + fname >>> C:\\Users\\tnguyen10\\OneDrive - Deloitte (O365D)\\Documents\\GitHub\\Thesis\\data\\articles\\mondelez.PDF\n",
      "fname >>> mondelez.PDF\n",
      "path + fname >>> C:\\Users\\tnguyen10\\OneDrive - Deloitte (O365D)\\Documents\\GitHub\\Thesis\\data\\articles\\ralph-lauren.PDF\n",
      "fname >>> ralph-lauren.PDF\n",
      "path + fname >>> C:\\Users\\tnguyen10\\OneDrive - Deloitte (O365D)\\Documents\\GitHub\\Thesis\\data\\articles\\sonoco.PDF\n",
      "fname >>> sonoco.PDF\n",
      "==== End of jobs ====\n",
      "CPU times: total: 1.73 s\n",
      "Wall time: 1.79 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df = read_filelist(path_articles)\n",
    "print('==== End of jobs ====')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ed2beaf-8acf-44e7-a373-23b058c29611",
   "metadata": {},
   "source": [
    "# Further Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f31a58f9-ffc8-4f3f-a7fb-98220a1fbdc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "143b1b92-b5a2-4f43-b949-7dff41b7d1af",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_article = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8a3be101-13a0-4660-abd9-71cfd0a8d0b8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# remove any quotes and unusual characters\n",
    "df_article[\"sentence\"] = df_article[\"sentence\"].str.replace('\"','', regex = True)\n",
    "df_article[\"sentence\"] = df_article[\"sentence\"].replace(r'http\\S+|\\[.\\]:?|www\\S+|\\w+/\\S+|\\w+-\\w+-\\S+|\\[|\\]','',regex = True).replace(r'^\\s+|\\s+$','',regex=True).replace(r'\\s{2,}',' ',regex=True)\n",
    "df_article['sentence'] = df_article['sentence'].str.replace('Nestl ', 'Nestle ')\n",
    "df_article['sentence'] = df_article['sentence'].str.replace('Mondel z', 'Mondelez')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d8e420d4-3df1-4f15-820e-6a828a36c041",
   "metadata": {},
   "outputs": [],
   "source": [
    "# removes empty sentence rows\n",
    "df_article = df_article[df_article['sentence'].astype(bool)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3924b2db-0e71-48d7-8cfc-d45f198f952d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_article.reset_index(inplace = True, drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6f3c0fe0-16fa-459b-8ad3-0552850c3bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# my pdf package does not know how to deal with sentences that span across pages - define a funciton here, which will merge the two sentences following each other\n",
    "# if the previous one doesn't end with punctuation and the following starts with a lower case letter\n",
    "import string\n",
    "\n",
    "# define a function to check if a sentence ends with punctuation\n",
    "def ends_with_punctuation(s):\n",
    "    return s.strip()[-1] in string.punctuation\n",
    "\n",
    "# loop over each row in the DataFrame and concatenate the sentences as needed\n",
    "for i, row in df_article.iterrows():\n",
    "    # skip the first row as there is no previous row to compare with\n",
    "    if i == 0:\n",
    "        continue\n",
    "    \n",
    "    # get the current and previous sentences\n",
    "    prev_sentence = df_article.loc[i-1, 'sentence']\n",
    "    curr_sentence = df_article.loc[i, 'sentence']\n",
    "    \n",
    "    # check if the previous sentence ends with punctuation and the current sentence starts with a lowercase letter\n",
    "    if not ends_with_punctuation(prev_sentence) and curr_sentence[0].islower():\n",
    "        # concatenate the sentences with a space\n",
    "        df_article.at[i, 'sentence'] = prev_sentence + ' ' + curr_sentence\n",
    "        # drop the previous row\n",
    "        df_article.drop(i-1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fc95321e-f6de-4d72-8053-2a8da3a5fea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filtering based on word count\n",
    "df_article[\"word count\"] = [len(i) for i in df_article[\"sentence\"].str.split()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f38a85cf-0568-4387-a8d9-ad9ef44548db",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_article = df_article[df_article[\"word count\"] > 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7fb68b93-0a78-4af9-a2b4-45d546fb906f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_article = df_article[df_article[\"word count\"] < 100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "53391acf-4e93-4488-997d-7b53eb5b552c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_type</th>\n",
       "      <th>company</th>\n",
       "      <th>sentence</th>\n",
       "      <th>word count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>news</td>\n",
       "      <td>beiersdorf</td>\n",
       "      <td>This offers businesses in food and beverage, p...</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>news</td>\n",
       "      <td>beiersdorf</td>\n",
       "      <td>Designed with sustainability in mind, the tesa...</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>news</td>\n",
       "      <td>beiersdorf</td>\n",
       "      <td>To reduce the consumption of virgin plastic, u...</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>news</td>\n",
       "      <td>beiersdorf</td>\n",
       "      <td>70 percent of the polyethylene (PET) that make...</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>news</td>\n",
       "      <td>beiersdorf</td>\n",
       "      <td>The tape supports the circular economy and can...</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3944</th>\n",
       "      <td>news</td>\n",
       "      <td>sonoco</td>\n",
       "      <td>Newstex Authoritative Content is not read and ...</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3945</th>\n",
       "      <td>news</td>\n",
       "      <td>sonoco</td>\n",
       "      <td>Accordingly, neither Newstex nor its re-distri...</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3946</th>\n",
       "      <td>news</td>\n",
       "      <td>sonoco</td>\n",
       "      <td>The Newstex Authoritative Content shall be con...</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3947</th>\n",
       "      <td>news</td>\n",
       "      <td>sonoco</td>\n",
       "      <td>Accordingly, no warranties or other guarantees...</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3948</th>\n",
       "      <td>news</td>\n",
       "      <td>sonoco</td>\n",
       "      <td>Newstex and its re-distributors expressly rese...</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3807 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     doc_type     company                                           sentence  \\\n",
       "0        news  beiersdorf  This offers businesses in food and beverage, p...   \n",
       "1        news  beiersdorf  Designed with sustainability in mind, the tesa...   \n",
       "2        news  beiersdorf  To reduce the consumption of virgin plastic, u...   \n",
       "3        news  beiersdorf  70 percent of the polyethylene (PET) that make...   \n",
       "4        news  beiersdorf  The tape supports the circular economy and can...   \n",
       "...       ...         ...                                                ...   \n",
       "3944     news      sonoco  Newstex Authoritative Content is not read and ...   \n",
       "3945     news      sonoco  Accordingly, neither Newstex nor its re-distri...   \n",
       "3946     news      sonoco  The Newstex Authoritative Content shall be con...   \n",
       "3947     news      sonoco  Accordingly, no warranties or other guarantees...   \n",
       "3948     news      sonoco  Newstex and its re-distributors expressly rese...   \n",
       "\n",
       "      word count  \n",
       "0             15  \n",
       "1             23  \n",
       "2             24  \n",
       "3             16  \n",
       "4             19  \n",
       "...          ...  \n",
       "3944          12  \n",
       "3945          40  \n",
       "3946          12  \n",
       "3947          25  \n",
       "3948          17  \n",
       "\n",
       "[3807 rows x 4 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define a function to check if a sentence is comprised of more than half uppercase characters\n",
    "def is_mostly_uppercase(sentence):\n",
    "    return sum(1 for c in sentence if c.isupper()) / len(sentence) > 0.5\n",
    "\n",
    "# apply the function to the 'sentence' column and filter out the rows where the condition is True\n",
    "df_article = df_article[~df_article['sentence'].apply(is_mostly_uppercase)]\n",
    "\n",
    "# print the resulting dataframe\n",
    "df_article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b9180634-7c88-4140-a564-077c9e859978",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_article.reset_index(inplace = True, drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "28074afc-21cf-4818-b0a1-722afca01376",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_article.drop_duplicates(subset = ['sentence'], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "293f43f4-d59c-4dc2-9dcb-06232cdc53f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_article.to_csv(fname_out, index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
