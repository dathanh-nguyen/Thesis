{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "614c994f-ad2b-4e61-8b35-fc5314986f0c",
   "metadata": {},
   "source": [
    "# Set-up and loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de441155-9dd7-45d9-91fc-e5f73188d5c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util, models\n",
    "from transformers import pipeline, AutoTokenizer, AutoModel, AutoModelForSequenceClassification\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from umap import UMAP\n",
    "from hdbscan import HDBSCAN\n",
    "import os\n",
    "from bertopic import BERTopic\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "from bertopic.representation import MaximalMarginalRelevance\n",
    "from sklearn.manifold import TSNE\n",
    "import plotly.express as px\n",
    "from tqdm.auto import tqdm\n",
    "import seaborn as sns\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "import re\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "# from octis.evaluation_metrics.diversity_metrics import TopicDiversity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4cf071a-9386-4ce7-9d32-9775904c6276",
   "metadata": {},
   "source": [
    "## Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c20c7e0-e245-4908-afee-e72805d49bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining data paths\n",
    "path_data = '..\\\\data_structured'\n",
    "path_reports = '..\\\\data\\\\reports'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b2fd46-f94e-4fcd-80bf-a1d3bf006c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a list of company names in our dataset to include in our vectorizer as stop words later + to remove them from text to create anonymized embeddings\n",
    "sample = os.listdir(path_reports)\n",
    "# converting to panda series because I find it easier to manipulate\n",
    "company_list = [word.split('.')[0] for word in sample]\n",
    "company_list = pd.Series(company_list)\n",
    "publisher_list = company_list.str.replace('-',' ') #so make it two words\n",
    "publisher_list = publisher_list.str.replace('ford motor', 'ford')\n",
    "publisher_list = publisher_list.str.replace('p&g', 'procter')\n",
    "names_list = publisher_list.tolist()\n",
    "names_list.append('p&g')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "692530d8-9d18-4b3a-bf8e-5130b54b7924",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading all the necessary data\n",
    "df_report = pd.read_csv(os.path.join(path_data,'report_sentences.csv'))\n",
    "df_pdf = pd.read_csv(os.path.join(path_data,'article_sentences_pdf.csv'))\n",
    "df_gnews = pd.read_csv(os.path.join(path_data,'article_sentences_gnews.csv'))\n",
    "df_article = pd.concat([df_pdf,df_gnews])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "851dfc7d-0552-48b5-87a5-71a09f39d18c",
   "metadata": {},
   "source": [
    "## Data pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a24da7b-997e-4f06-899a-312962869714",
   "metadata": {},
   "source": [
    "We apply the detailed pre-processing steps here in terms of generating the sentence embeddings. We also filter the data on character length as described in the thesis. Finally, we also 'anonymize' the sentences here by removing the company names so BERTopic has an easier time creating clusters without creating ones based on companies. This dataset is also used for the subsequent analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e2c7b79-0f2c-42e0-b143-34edec401a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining our sentence embedding model\n",
    "sent_embedder = SentenceTransformer('all-MiniLM-L6-v2', device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25d091b4-3ac4-4c63-abbc-ec575afb358c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_article.drop_duplicates(subset = ['sentence'], inplace = True)\n",
    "df_article.reset_index(inplace = True, drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6497c87-a151-4b84-8695-4e33ee470344",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rep = df_report.copy()\n",
    "df_art = df_article.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56335fda-1d8b-4a04-8850-b3bf5bb46f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for generating embeddings (useful for testing out later):\n",
    "def gen_embeddings(df,model):\n",
    "    sentences = df['sentence'].tolist()\n",
    "    return model.encode(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d500fb49-18ad-42cc-9814-d64b08b5f968",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# creating embeddings for the report sentences and storing them in a new column\n",
    "df_rep['embeddings'] = list(gen_embeddings(df_rep,sent_embedder))\n",
    "df_art['embeddings'] = list(gen_embeddings(df_art, sent_embedder))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "923dca8d-1d86-4597-adac-f5e8bafaec2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combining the two datasets in preparation for the clustering\n",
    "df_comb = pd.concat([df_art, df_rep])\n",
    "\n",
    "# filtering and applying any transformations necessary:\n",
    "# creating a character length variable to filter on text longer than 20 chars\n",
    "df_comb['char_length'] = df_comb['sentence'].apply(lambda x: len(x))\n",
    "df_comb = df_comb[(df_comb['char_length'] > 20)]\n",
    "df_comb.drop_duplicates(subset = ['sentence'], inplace = True)\n",
    "\n",
    "# Escaping the names_list for safe inclusion in the regular expression pattern\n",
    "escaped_names = [re.escape(name) for name in names_list]\n",
    "\n",
    "# Constructing the modified regular expression pattern\n",
    "pattern = r\"\\b(?:{})(?:'s)?\\b\".format(\"|\".join(escaped_names))\n",
    "\n",
    "# Replace company names with 'company' using the regular expression pattern\n",
    "df_comb['anon_sentence'] = df_comb['sentence'].str.replace(pattern, 'the company', case=False, regex=True)\n",
    "\n",
    "df_comb.reset_index(inplace = True, drop = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aedcac37-e2b2-4a48-90ca-dce38582619a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the sentence embeddings and doc types from the dataframe\n",
    "embeddings = np.array(df_comb[\"embeddings\"].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d8df1c5-a0ad-4cf0-9ff8-3ebe6678da08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating new embeddings based on the 'anonymized' sentences (without company names) - let's see how BERTopic handles it now\n",
    "df_comb['anon_embeddings'] = list(gen_embeddings(df_comb, sent_embedder))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b2673f-2156-496d-9036-a3183b6b124e",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = np.array(df_comb[\"anon_embeddings\"].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a9baa3b-df70-4a6e-ac85-2cabd2a98fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_comb = pd.to_pickle(os.path.join(path_data, 'comb.pkl'))\n",
    "# df_comb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cffb6bcb-1277-4f87-a510-53acff14b84a",
   "metadata": {},
   "source": [
    "# Descriptive Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56bc0008-d22c-4ffa-a1e5-a04b554fe23a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a histogram of the word count\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.hist(df_comb['word count'], bins=30, color='skyblue')\n",
    "plt.title('Word Count Distribution')\n",
    "plt.xlabel('Word Count')\n",
    "plt.ylabel('Frequency')\n",
    "plt.savefig('word_count_hist.png', dpi=300)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffeff762-f793-4169-89ec-c89363b6d4b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group the data by company, doc_type, and count the number of sentences\n",
    "df_by_company_doc_type = df_comb.groupby(['company', 'doc_type'])['sentence'].count().reset_index()\n",
    "\n",
    "# Pivot the dataframe to create separate columns for each doc_type\n",
    "df_pivot = df_by_company_doc_type.pivot(index='company', columns='doc_type', values='sentence')\n",
    "\n",
    "# Create a line chart of the number of sentences by company and doc_type\n",
    "plt.figure(figsize=(20,5));\n",
    "df_pivot.plot.line(color=['red', 'blue'], marker='o', markersize=5)\n",
    "plt.title('Number of Sentences by Company and Doc Type')\n",
    "plt.xlabel('Company')\n",
    "plt.ylabel('Number of Sentences')\n",
    "plt.legend(title='Doc Type')\n",
    "\n",
    "# Save the plot as a PNG file\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8b2c67f-3ecd-4483-b8eb-ae3c1e57a32b",
   "metadata": {},
   "source": [
    "# BERTopic Set-up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd0b945e-1c4f-400c-91a2-4a49647a48c4",
   "metadata": {},
   "source": [
    "## Dimensionality Reduction - default UMAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ac6109-8785-44b1-a148-2416c2d4605b",
   "metadata": {},
   "outputs": [],
   "source": [
    "umap2d = UMAP(n_components = 2, init = 'random', random_state = 0)\n",
    "# umap3d = UMAP(n_components = 3, init = 'random', random_state = 0)\n",
    "\n",
    "proj_2d = umap2d.fit_transform(embeddings)\n",
    "# proj_3d = umap3d.fit_transform(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bdea811-db05-40b2-8c00-3d2081a7b04a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig2d = px.scatter(\n",
    "    proj_2d, x = 0, y = 1,\n",
    "    width = 1000, height = 700\n",
    ")\n",
    "# fig3d = px.scatter_3d(\n",
    "#     proj_3d, x = 0, y = 1, z = 2,\n",
    "#     color = df_comb.doc_type, labels = {'color': 'doc_type'}\n",
    "# )\n",
    "\n",
    "fig2d.show()\n",
    "# fig3d.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bad3bdf6-6b13-4121-a42f-e8441e4eb219",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# comparing it to TSNE dimensionality reduction - takes quite a while so commented out \n",
    "# tsne2d = TSNE(n_components = 2, random_state = 0)\n",
    "# tsne3d = TSNE(n_components = 3, random_state = 0)\n",
    "\n",
    "# proj_2d_tsne = tsne2d.fit_transform(embeddings)\n",
    "# proj_3d_tsne = tsne3d.fit_transform(embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "847810f8-0374-46da-89fd-f12e58234d0a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# fig2d_tsne = px.scatter(\n",
    "#     proj_2d_tsne, x = 0, y = 1, \n",
    "#     color = df_comb.doc_type, labels = {'color': 'doc_type'}\n",
    "# )\n",
    "\n",
    "# # fig3d_tsne = px.scatter_3d(\n",
    "# #     proj_3d_tsne, x = 0, y = 1, z = 2,\n",
    "# #     color = df_comb.doc_type, labels = {'color': 'doc_type'}\n",
    "# # )\n",
    "\n",
    "# fig2d_tsne.show()\n",
    "# # fig3d_tsne.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "355bd75d-d026-4988-820c-5b9660e34db7",
   "metadata": {},
   "source": [
    "Based on the above visualizations, we can see that unlike Boelders, we do not require to overlap the sentence embeddings over each other, as there are not large semantic differences between the news articles and the reports. The jargon and language seems to be mostly the same, as such we can just use the original embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8813f8c-a8e0-4520-9028-ae3686624a86",
   "metadata": {},
   "source": [
    "### Testing UMAP parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "371c6704-f403-41cb-b27f-64b873b66afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, 3, figsize=(21, 10))\n",
    "nns = [5,10, 15, 30, 50, 100]\n",
    "#2, 3, 4\n",
    "i, j = 0, 0\n",
    "for n_neighbors in tqdm(nns):\n",
    "    fit = UMAP(n_neighbors=n_neighbors, random_state = 0)\n",
    "    u = fit.fit_transform(embeddings)\n",
    "    sns.scatterplot(x=u[:,0], y=u[:,1], ax=ax[j, i])\n",
    "    ax[j, i].set_title(f'n={n_neighbors}')\n",
    "    if i < 2: i += 1\n",
    "    else: i = 0; j += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2351ecae-d341-4116-833a-0a0b8b194ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2D representation\n",
    "fit2d = UMAP(n_neighbors=10, n_components=2, min_dist=0.0, init = 'random', metric = 'cosine', random_state = 0)\n",
    "u2d = fit2d.fit_transform(embeddings)\n",
    "\n",
    "fig2d = px.scatter(\n",
    "    x=u2d[:,0], y=u2d[:,1],\n",
    "    width = 1000, height = 700\n",
    ")\n",
    "\n",
    "fig2d.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cf67447-517a-45ea-8d2f-711b30484235",
   "metadata": {},
   "source": [
    "# Running BERTopic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a687351-7243-4fe1-b9b3-b7317a3e61d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = df_comb['anon_sentence']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f622186-8fd8-4537-ace5-8013d9bb516e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce dimensionality of embeddings, this step is necessary for later 2-dimensional representations of the clusters:\n",
    "reduced_embeddings = UMAP(n_neighbors=10, n_components=2, min_dist=0.0,init = 'random', metric = 'cosine', random_state = 0).fit_transform(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a02a5b6-5f5c-444e-b983-702ce379d1cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting stopwords for the TF-IDF representation\n",
    "stopwords = list(stopwords.words('english')) + ['company','coca','cola'] + names_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa25b91-b7da-424d-b01d-97d97535472b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# customizing parts of the BERTopic pipeline\n",
    "# vectorizer\n",
    "vectorizer_model = CountVectorizer(stop_words = stopwords)\n",
    "\n",
    "# umap\n",
    "umap_model = UMAP(n_neighbors=10, n_components=5, min_dist=0.0, init = 'random', metric = 'cosine', random_state = 0)\n",
    "\n",
    "# hdbscan\n",
    "hdbscan_model = HDBSCAN(min_cluster_size = 200, metric='euclidean', prediction_data=True)\n",
    "# 200 has decent results\n",
    "# min_samples\n",
    "# diversity topic words\n",
    "\n",
    "# this is to get more distinct topics\n",
    "representation_model = MaximalMarginalRelevance(diversity=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f66707-5dfb-4224-85ac-f5c9e1815d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining the topic model \n",
    "topic_model = BERTopic(\n",
    "    embedding_model = sent_embedder,\n",
    "    vectorizer_model = vectorizer_model,\n",
    "    umap_model = umap_model,\n",
    "    hdbscan_model = hdbscan_model,\n",
    "    language=\"english\", \n",
    "    representation_model = representation_model, #diversify topic words\n",
    "    calculate_probabilities=True, \n",
    "    verbose=True, \n",
    ")\n",
    "topics, probs = topic_model.fit_transform(docs, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc016d63-9a02-4d86-bf7d-e3a6e8adbb80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# optional visualizations\n",
    "# topic_model.visualize_barchart(top_n_topics=10, n_words = 10, height = 400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "666fd09c-89c4-4fef-ae01-8841b8e4f534",
   "metadata": {},
   "outputs": [],
   "source": [
    "# topic_model.visualize_documents(docs, reduced_embeddings=reduced_embeddings, \n",
    "#                                 hide_document_hover=True, hide_annotations=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90449b79-05b6-44b3-ae2f-6fb189807cce",
   "metadata": {},
   "source": [
    "## Topic Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c6d8786-5201-4944-90ba-a140cca6796a",
   "metadata": {},
   "outputs": [],
   "source": [
    "hierarchical_topics = topic_model.hierarchical_topics(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb9f081f-4fd8-4494-849e-16c5e1dab71f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a hierarchical tree\n",
    ">>> tree = topic_model.get_topic_tree(hierarchical_topics)\n",
    ">>> print(tree)\n",
    "\n",
    "# saf stands for sustainabile aviation fuel - together with topic 13 talks about the opportunities for sustainable fuel to reduce mostly emissions - can call it something like sustainable transportation? with dhl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3188162-c906-4b18-94c4-4a86e890f527",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.visualize_hierarchical_documents(docs, hierarchical_topics, reduced_embeddings=reduced_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7519f6f-a5eb-4a61-a381-b2b7c9e93c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now reduction of topics - we will merge them manually based ont he above visualizations\n",
    "topics_to_merge = [\n",
    "    [20,33,3,14], #plastic packaging and recycling\n",
    "    [12,4,7,51,60,24,59,30], #recycling, more focus on circular economy other than just plastic - waste management\n",
    "    [58,56,35,21,13], #regenerative agriculture/sustainable agriculture\n",
    "    [8,23,27], #water, biodiversity, deforestation - could be called nature preservation\n",
    "    [43,16], #sustainability (tech) innovation\n",
    "    [5,28], #climate change\n",
    "    [1,6,17,22], #sustainable leadership/governance\n",
    "    [29,11,25,19], #sustainable fuels/transportations\n",
    "    [39,40,2,15,53,61,45,47,30,32], #decarbonisation,emission reductions - include 47? (air pollution), 30? (chemical substances)\n",
    "    [18,34,46]#EV's\n",
    "] \n",
    "topic_model.merge_topics(docs, topics_to_merge)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "077c5474-3acc-493d-b185-c69d711925db",
   "metadata": {},
   "source": [
    "## Outlier Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4551ae6f-026a-4b14-abab-f425a5af6d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I saved and loaded the topic model here to not have to re-run it every time\n",
    "# topic_model.save('merged_model')\n",
    "# topic_model = BERTopic.load('merged_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f904cdcd-bbbb-4870-9b13-4620f99020c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = topic_model.topics_\n",
    "probs = topic_model.probabilities_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5beb4ff8-5c53-4f2b-8900-5c5dc5692954",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.get_topic_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "901c90f9-0eed-4689-bd12-88063a379f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#new_topics_probs = topic_model.reduce_outliers(docs, topics, probabilities=probs, strategy=\"probabilities\")\n",
    "#new_topics_tfidf = topic_model.reduce_outliers(docs, topics, strategy=\"c-tf-idf\", threshold = 0.1)\n",
    "new_topics_dist = topic_model.reduce_outliers(docs, topics, strategy = \"distributions\", threshold = 0.08)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd4994b2-d008-4638-8595-a7d59661d6bc",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### HDBSCAN Probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83460554-3680-464b-8706-8145921cda5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.update_topics(docs, topics=new_topics_probs, vectorizer_model = vectorizer_model, representation_model = representation_model)\n",
    "topic_model.visualize_documents(docs, reduced_embeddings=reduced_embeddings, \n",
    "                                hide_document_hover=True, hide_annotations=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89aea070-332e-43b2-adf9-8c73a2793985",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.get_topic_info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a6f37f7-2bff-4bcb-8bd7-fffd1cf7ee95",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### C-TF-IDF similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb594d73-ce64-44bc-be25-a1a56f74dff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.update_topics(docs, topics=new_topics_tfidf, vectorizer_model = vectorizer_model, representation_model = representation_model)\n",
    "topic_model.visualize_documents(docs, reduced_embeddings=reduced_embeddings, \n",
    "                                hide_document_hover=True, hide_annotations=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e3d9e4-71ff-4880-bbd9-3af1e5844355",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.get_topic_info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b8d18c6-974c-4062-9e7c-535f4a1003e6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Using Sentence and Topic Embedding Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e246b87c-b843-408b-8ebc-9ac1c4823cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#topic_model.update_topics(docs, topics=new_topics_embed, vectorizer_model = vectorizer_model,representation_model = representation_model)\n",
    "topic_model.visualize_documents(docs, reduced_embeddings=reduced_embeddings, \n",
    "                                hide_document_hover=False, hide_annotations=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6222048f-db22-422a-8a93-9f9b5f22597e",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.get_topic_info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b32d04f-ad46-4a2b-9949-89da9a627913",
   "metadata": {},
   "source": [
    "### Using topic distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95cebdc0-be9e-43e1-87ed-c53640f53556",
   "metadata": {},
   "outputs": [],
   "source": [
    "# updating the topics based on the reduction of outliers using topic distributions\n",
    "topic_model.update_topics(docs, topics=new_topics_dist, vectorizer_model = vectorizer_model, representation_model = representation_model) #  representation_model = representation_model\n",
    "topic_model.visualize_documents(docs, reduced_embeddings=reduced_embeddings, \n",
    "                                hide_document_hover=True, hide_annotations=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cbbf7cd-5973-4f2d-b9f5-97e90c6e5852",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a new column with the topics\n",
    "topics = topic_model.topics_\n",
    "df_comb['topics'] = topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b46b58-7080-4678-82b4-e76babdaafe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# second merging after taking a look at the reports\n",
    "topics_to_merge = [\n",
    "    [26,10], #green cars/car production\n",
    "    [14,3], #pollution lawsuits included\n",
    "    \n",
    "] \n",
    "topic_model.merge_topics(docs, topics_to_merge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e496d82-98ba-484a-8d14-e81f5a166d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.get_topic_info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a79b99c-a046-45c0-8b6f-f5be1f278349",
   "metadata": {},
   "source": [
    "# Generating Word Clouds and Other Visualizations for the Thesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a844ff-ac5d-4487-b9a1-9b7b4ba8d6d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "\n",
    "def create_wordcloud(model, topic, save_path = None):\n",
    "    # creating the text based on TF-IDF keywords to create the wordclouds\n",
    "    text = {word: value for word, value in model.get_topic(topic)}\n",
    "    wc = WordCloud(background_color=\"white\", max_words=1000)\n",
    "    wc.generate_from_frequencies(text)\n",
    "    plt.imshow(wc, interpolation=\"bilinear\")\n",
    "    plt.axis(\"off\")\n",
    "    if save_path:\n",
    "        plt.savefig(os.path.join(save_path, f\"wordcloud_{topic}.png\"))\n",
    "    else:\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c02442c1-c739-4ccd-bde9-168fff9b29a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sust_topics = [0,1,2,3,4,5,6,7,8,10,11,20,25]\n",
    "save_path = \".\\\\visualizations\"\n",
    "\n",
    "for i in sust_topics:\n",
    "    create_wordcloud(topic_model, i, save_path = save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb47abdd-3f62-47ad-80b0-012fccf0802c",
   "metadata": {},
   "source": [
    "## Creating Heatmaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "198dec6a-d507-4813-8c42-0b78f00932b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_labelling = {0: \"ESG Governance\", 1: \"Emission Reduction\", 2: \"Waste Management\", 3: \"Renewable Energy\", 4: \"Plastics Recycling\", 5: \"Electrical Vehicles\", 6:\"Climate Change Risk Mitigation\", 7: \"Nature Conservation\", 8: \"Green Transportation\",\n",
    "                   10: \"Sustainable Agriculture\", 11:\"Sustainable Innovation\", 20:\"Sustainable Finance\", 25:\"Greenwashing Accusations\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "261056d3-c2ff-4264-8f2c-f655621364ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_analyze = df_comb[df_comb['topics'].isin(sust_topics)]\n",
    "df_analyze.reset_index(inplace=True, drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "344c9f41-fb68-483a-9c5d-15245b81c085",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_analyze['topic_labels'] = df_analyze['topics'].apply(lambda x: topic_labelling[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f45ce04-2d04-4025-9142-40dbcda64f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the share of sentences per topic label and company\n",
    "df_share = df_analyze.groupby(['company', 'topic_labels'])['sentence'].count() / df_analyze.groupby('company')['sentence'].count()\n",
    "df_share = df_share.unstack().T\n",
    "\n",
    "# Create the heatmap\n",
    "plt.figure(figsize=(19, 13))\n",
    "sns.heatmap(df_share, cmap='PuRd', annot=False)\n",
    "plt.title('Share of Sentences per Topic Label and Company')\n",
    "plt.xlabel('Company')\n",
    "plt.ylabel('Topic Label')\n",
    "\n",
    "# # Save the plot as a PNG file\n",
    "plt.savefig('heatmap.png', dpi=500)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
