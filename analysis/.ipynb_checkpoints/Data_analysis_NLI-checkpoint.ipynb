{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aab75e6c-d163-4af9-b891-e4ed132b0aaa",
   "metadata": {},
   "source": [
    "This notebook calculates the potential greenwashing score by attempting to verify claims companies make in their CSR reports through the usage of companies' news coverage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c66c81ae-bf25-4c7e-aedc-83cdb2540b23",
   "metadata": {},
   "source": [
    "# Set-up and loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e7ea0dd-fb65-491b-99fc-4fb59463c269",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline, AutoTokenizer, AutoModel, AutoModelForSequenceClassification\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer, util, models\n",
    "import os\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e060b6-5a3d-4b80-b48f-703c6ce3effd",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ead8ee9-829b-4667-a182-9dce6cac5945",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining the data path\n",
    "path_data = '..\\\\data_structured'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8606a26-1cc5-4a94-bb56-562dbe6a32d8",
   "metadata": {},
   "source": [
    "## Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1908e6a8-38eb-429d-8b2c-f1a70e339679",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_comb = pd.read_pickle(os.path.join(path_data, 'comb.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63bfb2d4-f31a-444c-95ab-3b1dae145054",
   "metadata": {},
   "outputs": [],
   "source": [
    "sust_topics = [0,1,2,3,4,5,6,7,8,10,11,20,25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a69b0cc0-985c-4273-8e32-9eae185ff80b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_analyze = df_comb[df_comb['topics'].isin(sust_topics)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "592184b6-bf51-4875-91f1-8915d324a59b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_report = df_analyze[df_analyze['doc_type']=='report']\n",
    "df_article = df_analyze[df_analyze['doc_type']=='news']\n",
    "df_report.reset_index(drop = True, inplace = True)\n",
    "df_article.reset_index(drop = True, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b9bc05c-484a-4081-9493-099b1b069b32",
   "metadata": {},
   "source": [
    "# Claim Verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f99b2683-8ac1-49a8-bf58-9c2871cd6d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "claim_checker = pipeline(model = \"climatebert/environmental-claims\",  device = 0, batch_size = 64) # claim identification\n",
    "sem_search = SentenceTransformer('all-MiniLM-L6-v2', device='cuda') # evidence sentence selection\n",
    "nli_model = \"MoritzLaurer/DeBERTa-v3-large-mnli-fever-anli-ling-wanli\" # inference analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7625233a-959f-4dcf-b9e6-220ae96f5d33",
   "metadata": {},
   "source": [
    "The claim verification model consists of three stages - claim identification, evidence sentence selection and finally inference analysis. The three models above will help us achieve these three tasks. The ClimateBERT model is pre-trained to detect environmental and climate claims, semantic search will help us identify the 5 most relevant sentences from the corpus and finally the actual model can be used to check the entailment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a9d3e25-0f4f-44b6-a19e-61ac94148326",
   "metadata": {},
   "source": [
    "First, we apply the ClimateBERT model to identify environmental claims:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff3a59cf-a6ed-48ab-b0a5-e0dcc6bb656f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first part of the pipeline - identifying claims\n",
    "sentences = df_report['sentence'].tolist()  # Convert the column to a list\n",
    "\n",
    "results = claim_checker(sentences)\n",
    "df_report['claim'] = [result['label'] for result in results]\n",
    "df_report['claim_probability'] = [result['score'] for result in results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7481b47-28b4-427e-adfa-df700fb71c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_claims = df_report[df_report['claim']=='yes']\n",
    "df_claims.reset_index(inplace = True, drop = True)\n",
    "df_claims.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8728f552-a3b8-49b1-83d9-3cf96800e9e7",
   "metadata": {},
   "source": [
    "Since this took a while I will also pickle these to save my progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4ee4f17-52d7-4072-9765-91a2708dbf37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_article.to_pickle('art.pkl')\n",
    "# df_claims.to_pickle('claims.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75201a1e-dd5c-450a-a1e8-80bda9135b53",
   "metadata": {
    "tags": []
   },
   "source": [
    "Sentence transformers has a utility called semantic search which can be used to do find top 5 most similar sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "810a93c3-b6db-4a3c-a476-750d808090c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# for i,row in df_claims.iterrows():\n",
    "#     query_embedding = row['embeddings']\n",
    "#     company = row['company']\n",
    "#     # search only the article embeddings/sentences of the specific company\n",
    "#     corpus_embeddings = df_article[df_article['company']==company]['embeddings'].values\n",
    "#     top_5 = util.semantic_search(torch.Tensor(query_embedding), torch.Tensor(np.array(list(corpus_embeddings))), top_k = 5)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33fbd249-9b35-44c9-bc13-753777ab9925",
   "metadata": {},
   "source": [
    "Let us now create a new dataframe based on df_claims, which will store the same information as this dataframe, but will also additionally hold the top 5 most similar sentences in a separate column, as well as whether these sentences entail, contradict or are neutral towards each other. I use the MoritzLaurer NLI model for this purpose as it states that it is the best performing NLI model on the HuggingFace hub as of June 2022. The code used for the classification is mostly copied from the HuggingFace transformers website and modified for our purposes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "548a451f-6ccf-439d-913d-9e556958314b",
   "metadata": {},
   "source": [
    "We repeat the same code as above but expand upon it further:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efdee7d5-1982-48f1-8df6-c71ab4ee1fcd",
   "metadata": {},
   "source": [
    "## Firm-Level Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b225b238-35b4-417d-a925-9b690b923ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(nli_model)\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(nli_model).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4791e7e9-68eb-4e5c-b12d-28014c474ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # making lists to store values for the new columns\n",
    "top_sentences_column = []\n",
    "predictions = []\n",
    "probabilities = []\n",
    "\n",
    "# we run a for loop for each claim in the df_entailment dataset and check the validity of the claim\n",
    "for i,row in df_claims.iterrows():\n",
    "    # define our query (i.e. claim) and the company it's related to \n",
    "    query_embedding = row['embeddings']\n",
    "    company = row['company']\n",
    "    # search only the article embeddings/sentences of the specific company\n",
    "    corpus_embeddings = df_article[df_article['company']==company]['embeddings'].values\n",
    "    top_5 = util.semantic_search(torch.Tensor(query_embedding), torch.Tensor(np.array(list(corpus_embeddings))), top_k = 5)\n",
    "    # define a list to hold our top sentences and predictions to add these as a new variable after the loop\n",
    "    hard_predictions = []\n",
    "    top_sentences = []\n",
    "    soft_predictions =[]\n",
    "    for sentence in  top_5[0]:\n",
    "        # the premise is the claim\n",
    "        premise = row['sentence']\n",
    "        # the hypothesis is the sentence from the article(identified using the corpus id, which gives us the index of the sentence)\n",
    "        hypothesis = df_article[df_article['company']==company]['sentence'].values[sentence['corpus_id']]\n",
    "        tokens = tokenizer(premise, hypothesis, truncation=True, return_tensors=\"pt\")\n",
    "        output = model(tokens[\"input_ids\"].to(device))  # device = \"cuda:0\" or \"cpu\"\n",
    "        soft_prediction = torch.softmax(output[\"logits\"][0], -1)\n",
    "        label_names = [\"entailment\", \"neutral\", \"contradiction\"]\n",
    "        hard_prediction = label_names[torch.argmax(output[\"logits\"][0], -1).item()]\n",
    "        # append the different values to the correct list\n",
    "        top_sentences.append(hypothesis)\n",
    "        soft_predictions.append(max(torch.softmax(output[\"logits\"][0], -1).tolist()))\n",
    "        hard_predictions.append(hard_prediction)\n",
    "        \n",
    "    top_sentences_column.append(top_sentences)\n",
    "    predictions.append(hard_predictions)\n",
    "    probabilities.append(soft_predictions)\n",
    "\n",
    "df_claims['top_sentences'] = top_sentences_column\n",
    "df_claims['predictions'] = predictions\n",
    "df_claims['probabilities'] = probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e68e5429-90fe-477e-866f-0174de88c18f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_frequent_category(categories_list):\n",
    "    counter = Counter(categories_list)\n",
    "    return counter.most_common(1)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef5e230-c8f8-4c6c-871b-c4473cc4ecf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def count_non_neutral(categories_list):\n",
    "#     counter = Counter(categories_list)\n",
    "#     for element,count in counter.items():\n",
    "#         if (count >= 2) & (element!='neutral'):\n",
    "#             return element\n",
    "#     return 'neutral'\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c21cf198-b146-4f76-b669-46ff75b75c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_claims['consensus'] = df_claims['predictions'].apply(most_frequent_category)\n",
    "df_claims['consensus'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b8800c7-ba7f-4ab7-8400-a5c675649eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_claims['consensus'] = df_claims['predictions'].apply(count_non_neutral)\n",
    "# df_claims['consensus'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a0e80b1-5cb7-4e51-b776-baedf7cb500d",
   "metadata": {},
   "outputs": [],
   "source": [
    "gw_database_total = {}\n",
    "for firm in set(df_claims['company']):\n",
    "     # calculate the average sentiment score for this firm for symbolic actions\n",
    "    ver_score = len(df_claims[(df_claims['company']==firm)&(df_claims['consensus']!='entailment')])/len(df_claims[df_claims['company']==firm])\n",
    "    gw_database_total[firm] = ver_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e4fb81-7f37-46d2-82d3-0583997f5da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gw_total = pd.DataFrame(gw_database_total.items(),columns=['company', 'verification_score'])\n",
    "scaler = MinMaxScaler()\n",
    "df_gw_total['verification_score'] = scaler.fit_transform(df_gw_total[['verification_score']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df42dc3c-cef3-46cc-8ce9-efb62158dc28",
   "metadata": {},
   "source": [
    "## Cluster-Level Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a535140d-27ad-4fef-a99e-e7b9e441042a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # making lists to store values for the new columns\n",
    "top_sentences_column = []\n",
    "predictions = []\n",
    "probabilities = []\n",
    "\n",
    "# we run a for loop for each claim in the df_entailment dataset and check the validity of the claim\n",
    "for i,row in df_claims.iterrows():\n",
    "    # define our query (i.e. claim) and the company it's related to \n",
    "    query_embedding = row['embeddings']\n",
    "    company = row['company']\n",
    "    topic = row['topics']\n",
    "    # search only the article embeddings/sentences of the specific company and topic\n",
    "    corpus_embeddings = df_article[(df_article['company']==company)&(df_article['topics']==topic)]['embeddings'].values\n",
    "    top_5 = util.semantic_search(torch.Tensor(query_embedding), torch.Tensor(np.array(list(corpus_embeddings))), top_k = 5)\n",
    "    # define a list to hold our top sentences and predictions to add these as a new variable after the loop\n",
    "    hard_predictions = []\n",
    "    top_sentences = []\n",
    "    soft_predictions =[]\n",
    "    for sentence in  top_5[0]:\n",
    "        # the premise is the claim\n",
    "        premise = row['sentence']\n",
    "        # the hypothesis is the sentence from the article(identified using the corpus id, which gives us the index of the sentence)\n",
    "        hypothesis = df_article[df_article['company']==company]['sentence'].values[sentence['corpus_id']]\n",
    "        tokens = tokenizer(premise, hypothesis, truncation=True, return_tensors=\"pt\")\n",
    "        output = model(tokens[\"input_ids\"].to(device))  # device = \"cuda:0\" or \"cpu\"\n",
    "        soft_prediction = torch.softmax(output[\"logits\"][0], -1)\n",
    "        label_names = [\"entailment\", \"neutral\", \"contradiction\"]\n",
    "        hard_prediction = label_names[torch.argmax(output[\"logits\"][0], -1).item()]\n",
    "        # append the different values to the correct list\n",
    "        top_sentences.append(hypothesis)\n",
    "        soft_predictions.append(max(torch.softmax(output[\"logits\"][0], -1).tolist()))\n",
    "        hard_predictions.append(hard_prediction)\n",
    "        \n",
    "    top_sentences_column.append(top_sentences)\n",
    "    predictions.append(hard_predictions)\n",
    "    probabilities.append(soft_predictions)\n",
    "\n",
    "df_claims['top_sentences_cluster'] = top_sentences_column\n",
    "df_claims['predictions_cluster'] = predictions\n",
    "df_claims['probabilities_cluster'] = probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d401ab31-7d6d-4d52-8af6-b4468497e6f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_claims['consensus'] = df_claims['predictions'].apply(most_frequent_category)\n",
    "df_claims['consensus'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34ee52ab-0777-4afa-bd84-3bc9317ccb9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "gw_database = {}\n",
    "for cluster in set(df_claims['topics']):\n",
    "    firm_cluster_score = {}\n",
    "    for firm in set(df_claims['company']):\n",
    "        try:\n",
    "            firm_cluster_score[firm] = len(df_claims[(df_claims['company']==firm)&(df_claims['topics']==cluster)&(df_claims['consensus']!='entailment')])/len(df_claims[(df_claims['company']==firm)&(df_claims['topics']==cluster)])\n",
    "        except:\n",
    "            firm_cluster_score[firm] = np.nan\n",
    "     # calculate the average sentiment score for this firm for symbolic actions\n",
    "    gw_database[cluster] = firm_cluster_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeaf0d89-51f7-4925-9c8b-84e49c9ee0ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gw = pd.DataFrame.from_dict(gw_database)\n",
    "df_gw['verification_cluster'] = df_gw[sust_topics].mean(axis = 1, skipna = True)\n",
    "df_gw['verification_cluster'] = scaler.fit_transform(df_gw[['verification_cluster']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "211784e6-c002-45a7-9e23-ad4db50b519e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gw.reset_index(inplace = True)\n",
    "df_gw.rename(columns = {'index':'company'}, inplace = True)\n",
    "df_gw = df_gw[['company', 'verification_cluster']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "902aad29-f7a9-4206-a8b0-fffbb491254b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_verification = pd.merge(df_gw_total, df_gw)\n",
    "df_verification.to_csv('verification_scores.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
