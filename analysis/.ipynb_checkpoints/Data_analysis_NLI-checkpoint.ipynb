{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c66c81ae-bf25-4c7e-aedc-83cdb2540b23",
   "metadata": {},
   "source": [
    "# Set-up and data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e7ea0dd-fb65-491b-99fc-4fb59463c269",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline, AutoTokenizer, AutoModel, AutoModelForSequenceClassification\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy.spatial.distance import cosine\n",
    "from sentence_transformers import SentenceTransformer, util, models\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ead8ee9-829b-4667-a182-9dce6cac5945",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_data = 'C:\\\\Users\\\\tnguyen10\\\\OneDrive - Deloitte (O365D)\\\\Documents\\\\GitHub\\\\Thesis\\\\data_structured'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8606a26-1cc5-4a94-bb56-562dbe6a32d8",
   "metadata": {},
   "source": [
    "## Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1908e6a8-38eb-429d-8b2c-f1a70e339679",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_report = pd.read_csv(os.path.join(path_data,'report_sentences.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5a4bada9-bf96-4bc9-87fe-c14def1871a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pdf = pd.read_csv(os.path.join(path_data,'article_sentences_pdf.csv'))\n",
    "df_gnews = pd.read_csv(os.path.join(path_data,'article_sentences_gnews.csv'))\n",
    "df_article = pd.concat([df_pdf,df_gnews])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bd1e7d02-ec01-4a0b-ac37-e329efa90d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_report = df_report[df_report[\"word count\"] > 5]\n",
    "df_report = df_report[df_report[\"word count\"] < 100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ee41a973-ef80-46de-b499-78fb30701713",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_report.rename(columns = {'fname':'company'},inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "592184b6-bf51-4875-91f1-8915d324a59b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_article = df_article[df_article[\"word count\"] > 5]\n",
    "df_article = df_article[df_article[\"word count\"] < 100]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b9bc05c-484a-4081-9493-099b1b069b32",
   "metadata": {},
   "source": [
    "# Applying the pre-trained models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f99b2683-8ac1-49a8-bf58-9c2871cd6d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "claim_checker = pipeline(model = \"climatebert/environmental-claims\")\n",
    "sem_search = SentenceTransformer('sentence-transformers/paraphrase-MiniLM-L6-v2')\n",
    "nli_model = \"MoritzLaurer/DeBERTa-v3-large-mnli-fever-anli-ling-wanli\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7625233a-959f-4dcf-b9e6-220ae96f5d33",
   "metadata": {},
   "source": [
    "The claim verification model consists of three stages - claim identification, evidence sentence selection and finally inference analysis. The three models above will help us achieve these three tasks. The ClimateBERT model is pre-trained to detect environmental and climate claims, semantic search will help us identify the 5 most relevant sentences from the corpus and finally the actual model can be used to check the entailment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a9d3e25-0f4f-44b6-a19e-61ac94148326",
   "metadata": {},
   "source": [
    "First, we apply the ClimateBERT model to identify environmental claims:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ff3a59cf-a6ed-48ab-b0a5-e0dcc6bb656f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_report['claim'] = df_report['sentence'].map(lambda x: claim_checker(x)[0]['label'])\n",
    "df_report['claim_score'] = df_report['sentence'].map(lambda x: claim_checker(x)[0]['score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e7481b47-28b4-427e-adfa-df700fb71c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_claims = df_report[df_report['claim']=='yes']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "721c316f-b561-4249-b7ab-47dc60a70ed0",
   "metadata": {},
   "source": [
    "Now we create the sentence embeddings using the semantic search model. These embeddings will be used by the sentence transformers package to find the top 5 most similar sentences from the article corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "63a36ad0-1a65-4caa-b9b2-9b85003f37a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tnguyen10\\AppData\\Local\\Temp\\ipykernel_25844\\2746180871.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_claims['embeddings'] = list(claims_embeddings)\n"
     ]
    }
   ],
   "source": [
    "claims_sent = df_claims['sentence'].tolist()\n",
    "claims_embeddings = sem_search.encode(claims_sent)\n",
    "df_claims['embeddings'] = list(claims_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b5999e0f-8c14-4aef-af13-788eb7988901",
   "metadata": {},
   "outputs": [],
   "source": [
    "article_sent = df_article['sentence'].tolist()\n",
    "article_embeddings = sem_search.encode(article_sent)\n",
    "df_article['embeddings'] = list(article_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8728f552-a3b8-49b1-83d9-3cf96800e9e7",
   "metadata": {},
   "source": [
    "Since this took a while I will also pickle these to save my progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c4ee4f17-52d7-4072-9765-91a2708dbf37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_article.to_pickle('art.pkl')\n",
    "# df_claims.to_pickle('claims.pkl')\n",
    "df_claims = pd.read_pickle('claims.pkl')\n",
    "df_article = pd.read_pickle('art.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75201a1e-dd5c-450a-a1e8-80bda9135b53",
   "metadata": {
    "tags": []
   },
   "source": [
    "Sentence transformers has a utility called semantic search which can be used to do this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "810a93c3-b6db-4a3c-a476-750d808090c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 15.6 ms\n",
      "Wall time: 23.4 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for i,row in df_claims.iterrows():\n",
    "    query_embedding = row['embeddings']\n",
    "    company = row['company']\n",
    "    # search only the article embeddings/sentences of the specific company\n",
    "    corpus_embeddings = df_article[df_article['company']==company]['embeddings'].values\n",
    "    top_5 = util.semantic_search(torch.Tensor(query_embedding), torch.Tensor(np.array(list(corpus_embeddings))), top_k = 5)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d8a73c87-b632-4938-b0ba-7838316bf398",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'corpus_id': 732, 'score': 0.6140859127044678},\n",
       "  {'corpus_id': 649, 'score': 0.6044427752494812},\n",
       "  {'corpus_id': 234, 'score': 0.5697278380393982},\n",
       "  {'corpus_id': 688, 'score': 0.5586557984352112},\n",
       "  {'corpus_id': 654, 'score': 0.5541017055511475}]]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33fbd249-9b35-44c9-bc13-753777ab9925",
   "metadata": {},
   "source": [
    "Let us now create a new dataframe based on df_claims, which will store the same information as this dataframe, but will also additionally hold the top 5 most similar sentences in a separate column, as well as whether these sentences entail, contradict or are neutral towards each other. I use the MoritzLaurer NLI model for this purpose as it states that it is the best performing NLI model as of June 2022. The code used for the classification is mostly copied from the HuggingFace transformers website and modified for our purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fecfe0c6-d3f5-412a-addd-a2f29c52eded",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_entailment = df_claims.copy()\n",
    "df_entailment.reset_index(inplace = True, drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7bb41abc-2c0a-48aa-9bd0-2783e70a8bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_entailment = df_entailment.reindex(df_entailment.columns.tolist() + ['top_sentences','predictions','probabilities'], axis=1)  # version > 0.20.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "afb20ca9-8aa8-49f1-b8ca-254f6aa7dd7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_sample = df_entailment[:5] \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "548a451f-6ccf-439d-913d-9e556958314b",
   "metadata": {},
   "source": [
    "We repeat the same code as above but expand upon it further:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b225b238-35b4-417d-a925-9b690b923ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(nli_model)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(nli_model)\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4791e7e9-68eb-4e5c-b12d-28014c474ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # making lists to store values for the new columns\n",
    "# top_sentences_column = []\n",
    "# predictions = []\n",
    "# probabilities = []\n",
    "\n",
    "# we run a for loop for each claim in the df_entailment dataset and check the validity of the claim\n",
    "for i,row in df_entailment[7022:].iterrows():\n",
    "    # define our query (i.e. claim) and the company it's related to \n",
    "    query_embedding = row['embeddings']\n",
    "    company = row['company']\n",
    "    # search only the article embeddings/sentences of the specific company\n",
    "    corpus_embeddings = df_article[df_article['company']==company]['embeddings'].values\n",
    "    top_5 = util.semantic_search(torch.Tensor(query_embedding), torch.Tensor(np.array(list(corpus_embeddings))), top_k = 5)\n",
    "    # define a list to hold our top sentences and predictions to add these as a new variable after the loop\n",
    "    hard_predictions = []\n",
    "    top_sentences = []\n",
    "    soft_predictions =[]\n",
    "    for sentence in  top_5[0]:\n",
    "        # the premise is the claim\n",
    "        premise = row['sentence']\n",
    "        # the hypothesis is the sentence from the article(identified using the corpus id, which gives us the index of the sentence)\n",
    "        hypothesis = df_article[df_article['company']==company]['sentence'].values[sentence['corpus_id']]\n",
    "        tokens = tokenizer(premise, hypothesis, truncation=True, return_tensors=\"pt\")\n",
    "        output = model(tokens[\"input_ids\"].to(device))  # device = \"cuda:0\" or \"cpu\"\n",
    "        soft_prediction = torch.softmax(output[\"logits\"][0], -1)\n",
    "        label_names = [\"entailment\", \"neutral\", \"contradiction\"]\n",
    "        hard_prediction = label_names[torch.argmax(output[\"logits\"][0], -1).item()]\n",
    "        # append the different values to the correct list\n",
    "        top_sentences.append(hypothesis)\n",
    "        soft_predictions.append(max(torch.softmax(output[\"logits\"][0], -1).tolist()))\n",
    "        hard_predictions.append(hard_prediction)\n",
    "    # # now add the different lists as new variables\n",
    "    # df_sample.at[i,'top_sentences'] = str(top_sentences)\n",
    "    # df_sample.at[i,'predictions'] = str(hard_predictions)\n",
    "    # df_sample.at[i,'probabilities'] = str(soft_predictions)\n",
    "    top_sentences_column.append(top_sentences)\n",
    "    predictions.append(hard_predictions)\n",
    "    probabilities.append(soft_predictions)\n",
    "\n",
    "df_entailment['top_sentences'] = top_sentences_column\n",
    "df_entailment['predictions'] = predictions\n",
    "df_entailment['probabilities'] = probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3adf1c12-64a3-4268-ba83-7d2a180de463",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11426"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "400e59a5-d705-4f5f-98a1-00a0896f30ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11426"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_entailment[7022:])+len(df_entailment[:7022])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e68e5429-90fe-477e-866f-0174de88c18f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def most_frequent_category(categories_list):\n",
    "    counter = Counter(categories_list)\n",
    "    return counter.most_common(1)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c21cf198-b146-4f76-b669-46ff75b75c87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "neutral          11317\n",
       "contradiction       63\n",
       "entailment          46\n",
       "Name: consensus, dtype: int64"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_entailment['consensus'] = df_entailment['predictions'].apply(most_frequent_category)\n",
    "df_entailment['consensus'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8b8800c7-ba7f-4ab7-8400-a5c675649eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_pickle('entailment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "61ebd350-d01e-400e-b0e8-949f579195b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_type</th>\n",
       "      <th>company</th>\n",
       "      <th>sentence</th>\n",
       "      <th>word count</th>\n",
       "      <th>claim</th>\n",
       "      <th>claim_score</th>\n",
       "      <th>embeddings</th>\n",
       "      <th>top_sentences</th>\n",
       "      <th>predictions</th>\n",
       "      <th>probabilities</th>\n",
       "      <th>consensus</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>report</td>\n",
       "      <td>abb</td>\n",
       "      <td>customers to deliver annual savings of 100 meg...</td>\n",
       "      <td>13</td>\n",
       "      <td>yes</td>\n",
       "      <td>0.993644</td>\n",
       "      <td>[-0.6807976, 0.4772148, -0.26226345, -0.029244...</td>\n",
       "      <td>[A key part of our 2030 sustainability strateg...</td>\n",
       "      <td>[neutral, neutral, neutral, contradiction, neu...</td>\n",
       "      <td>[0.9967696666717529, 0.9962621331214905, 0.995...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>report</td>\n",
       "      <td>abb</td>\n",
       "      <td>We found that 36 percent of our revenue in 202...</td>\n",
       "      <td>19</td>\n",
       "      <td>yes</td>\n",
       "      <td>0.949681</td>\n",
       "      <td>[0.23194747, 0.085391074, -0.19298553, 0.04695...</td>\n",
       "      <td>[SN: Company report shows that ABBs greenhouse...</td>\n",
       "      <td>[neutral, neutral, neutral, neutral, neutral]</td>\n",
       "      <td>[0.9966341853141785, 0.9991693496704102, 0.998...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>report</td>\n",
       "      <td>abb</td>\n",
       "      <td>We consider this to be a significant underesti...</td>\n",
       "      <td>33</td>\n",
       "      <td>yes</td>\n",
       "      <td>0.989664</td>\n",
       "      <td>[-0.14990805, 0.17751908, -0.098325394, 0.1222...</td>\n",
       "      <td>[Theres a whole range of solutions that we can...</td>\n",
       "      <td>[entailment, neutral, neutral, neutral, neutral]</td>\n",
       "      <td>[0.9093318581581116, 0.9912990927696228, 0.998...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>report</td>\n",
       "      <td>abb</td>\n",
       "      <td>A second goal of our 2030 sustainability strat...</td>\n",
       "      <td>19</td>\n",
       "      <td>yes</td>\n",
       "      <td>0.989790</td>\n",
       "      <td>[-0.24475533, 0.4377824, -0.21117015, -0.03091...</td>\n",
       "      <td>[A key part of our 2030 sustainability strateg...</td>\n",
       "      <td>[neutral, neutral, neutral, neutral, neutral]</td>\n",
       "      <td>[0.9479023814201355, 0.9969388246536255, 0.999...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>report</td>\n",
       "      <td>abb</td>\n",
       "      <td>In December 2021, we unveiled a new company-wi...</td>\n",
       "      <td>21</td>\n",
       "      <td>yes</td>\n",
       "      <td>0.992168</td>\n",
       "      <td>[-0.49132273, -0.4556106, -0.45783857, -0.3235...</td>\n",
       "      <td>[At ABB, weve set a target to take a circular ...</td>\n",
       "      <td>[neutral, neutral, neutral, neutral, neutral]</td>\n",
       "      <td>[0.9995922446250916, 0.8719384670257568, 0.955...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11421</th>\n",
       "      <td>report</td>\n",
       "      <td>walmart</td>\n",
       "      <td>To advance responsible recruitment across our ...</td>\n",
       "      <td>23</td>\n",
       "      <td>yes</td>\n",
       "      <td>0.825599</td>\n",
       "      <td>[-0.44181126, -0.49926084, -0.36026376, 0.1699...</td>\n",
       "      <td>[To promote human dignity, Walmart has also co...</td>\n",
       "      <td>[neutral, neutral, contradiction, neutral, neu...</td>\n",
       "      <td>[0.9993495345115662, 0.9984261989593506, 0.720...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11422</th>\n",
       "      <td>report</td>\n",
       "      <td>walmart</td>\n",
       "      <td>We also promote the adoption of best practices...</td>\n",
       "      <td>13</td>\n",
       "      <td>yes</td>\n",
       "      <td>0.792460</td>\n",
       "      <td>[-0.39208585, -0.40989313, -0.319746, -0.23068...</td>\n",
       "      <td>[Companies can also use it in the supply chain...</td>\n",
       "      <td>[neutral, neutral, neutral, neutral, neutral]</td>\n",
       "      <td>[0.996663510799408, 0.9982901215553284, 0.9987...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11423</th>\n",
       "      <td>report</td>\n",
       "      <td>walmart</td>\n",
       "      <td>For example, as of the end of FY2022, 69% of W...</td>\n",
       "      <td>41</td>\n",
       "      <td>yes</td>\n",
       "      <td>0.861281</td>\n",
       "      <td>[0.12809211, -0.2407749, -0.16362701, -0.09550...</td>\n",
       "      <td>[For example, Walmart aims to have 100% of its...</td>\n",
       "      <td>[contradiction, neutral, neutral, neutral, neu...</td>\n",
       "      <td>[0.6092495322227478, 0.9995492100715637, 0.999...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11424</th>\n",
       "      <td>report</td>\n",
       "      <td>walmart</td>\n",
       "      <td>To accelerate system-wide change, the Walmart ...</td>\n",
       "      <td>38</td>\n",
       "      <td>yes</td>\n",
       "      <td>0.913087</td>\n",
       "      <td>[-0.03436353, 0.11737306, 0.07235241, 0.195486...</td>\n",
       "      <td>[In terms of philanthropy, our Walmart foundat...</td>\n",
       "      <td>[neutral, neutral, neutral, neutral, neutral]</td>\n",
       "      <td>[0.5700876116752625, 0.9993413090705872, 0.997...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11425</th>\n",
       "      <td>report</td>\n",
       "      <td>walmart</td>\n",
       "      <td>Building on our experiences as a founding memb...</td>\n",
       "      <td>49</td>\n",
       "      <td>yes</td>\n",
       "      <td>0.760555</td>\n",
       "      <td>[-0.08230344, -0.20270939, -0.11352123, -0.045...</td>\n",
       "      <td>[That means elevating people and the resources...</td>\n",
       "      <td>[neutral, neutral, neutral, neutral, neutral]</td>\n",
       "      <td>[0.9953122138977051, 0.9984742999076843, 0.989...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11426 rows Ã— 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      doc_type  company                                           sentence  \\\n",
       "0       report      abb  customers to deliver annual savings of 100 meg...   \n",
       "1       report      abb  We found that 36 percent of our revenue in 202...   \n",
       "2       report      abb  We consider this to be a significant underesti...   \n",
       "3       report      abb  A second goal of our 2030 sustainability strat...   \n",
       "4       report      abb  In December 2021, we unveiled a new company-wi...   \n",
       "...        ...      ...                                                ...   \n",
       "11421   report  walmart  To advance responsible recruitment across our ...   \n",
       "11422   report  walmart  We also promote the adoption of best practices...   \n",
       "11423   report  walmart  For example, as of the end of FY2022, 69% of W...   \n",
       "11424   report  walmart  To accelerate system-wide change, the Walmart ...   \n",
       "11425   report  walmart  Building on our experiences as a founding memb...   \n",
       "\n",
       "       word count claim  claim_score  \\\n",
       "0              13   yes     0.993644   \n",
       "1              19   yes     0.949681   \n",
       "2              33   yes     0.989664   \n",
       "3              19   yes     0.989790   \n",
       "4              21   yes     0.992168   \n",
       "...           ...   ...          ...   \n",
       "11421          23   yes     0.825599   \n",
       "11422          13   yes     0.792460   \n",
       "11423          41   yes     0.861281   \n",
       "11424          38   yes     0.913087   \n",
       "11425          49   yes     0.760555   \n",
       "\n",
       "                                              embeddings  \\\n",
       "0      [-0.6807976, 0.4772148, -0.26226345, -0.029244...   \n",
       "1      [0.23194747, 0.085391074, -0.19298553, 0.04695...   \n",
       "2      [-0.14990805, 0.17751908, -0.098325394, 0.1222...   \n",
       "3      [-0.24475533, 0.4377824, -0.21117015, -0.03091...   \n",
       "4      [-0.49132273, -0.4556106, -0.45783857, -0.3235...   \n",
       "...                                                  ...   \n",
       "11421  [-0.44181126, -0.49926084, -0.36026376, 0.1699...   \n",
       "11422  [-0.39208585, -0.40989313, -0.319746, -0.23068...   \n",
       "11423  [0.12809211, -0.2407749, -0.16362701, -0.09550...   \n",
       "11424  [-0.03436353, 0.11737306, 0.07235241, 0.195486...   \n",
       "11425  [-0.08230344, -0.20270939, -0.11352123, -0.045...   \n",
       "\n",
       "                                           top_sentences  \\\n",
       "0      [A key part of our 2030 sustainability strateg...   \n",
       "1      [SN: Company report shows that ABBs greenhouse...   \n",
       "2      [Theres a whole range of solutions that we can...   \n",
       "3      [A key part of our 2030 sustainability strateg...   \n",
       "4      [At ABB, weve set a target to take a circular ...   \n",
       "...                                                  ...   \n",
       "11421  [To promote human dignity, Walmart has also co...   \n",
       "11422  [Companies can also use it in the supply chain...   \n",
       "11423  [For example, Walmart aims to have 100% of its...   \n",
       "11424  [In terms of philanthropy, our Walmart foundat...   \n",
       "11425  [That means elevating people and the resources...   \n",
       "\n",
       "                                             predictions  \\\n",
       "0      [neutral, neutral, neutral, contradiction, neu...   \n",
       "1          [neutral, neutral, neutral, neutral, neutral]   \n",
       "2       [entailment, neutral, neutral, neutral, neutral]   \n",
       "3          [neutral, neutral, neutral, neutral, neutral]   \n",
       "4          [neutral, neutral, neutral, neutral, neutral]   \n",
       "...                                                  ...   \n",
       "11421  [neutral, neutral, contradiction, neutral, neu...   \n",
       "11422      [neutral, neutral, neutral, neutral, neutral]   \n",
       "11423  [contradiction, neutral, neutral, neutral, neu...   \n",
       "11424      [neutral, neutral, neutral, neutral, neutral]   \n",
       "11425      [neutral, neutral, neutral, neutral, neutral]   \n",
       "\n",
       "                                           probabilities consensus  \n",
       "0      [0.9967696666717529, 0.9962621331214905, 0.995...   neutral  \n",
       "1      [0.9966341853141785, 0.9991693496704102, 0.998...   neutral  \n",
       "2      [0.9093318581581116, 0.9912990927696228, 0.998...   neutral  \n",
       "3      [0.9479023814201355, 0.9969388246536255, 0.999...   neutral  \n",
       "4      [0.9995922446250916, 0.8719384670257568, 0.955...   neutral  \n",
       "...                                                  ...       ...  \n",
       "11421  [0.9993495345115662, 0.9984261989593506, 0.720...   neutral  \n",
       "11422  [0.996663510799408, 0.9982901215553284, 0.9987...   neutral  \n",
       "11423  [0.6092495322227478, 0.9995492100715637, 0.999...   neutral  \n",
       "11424  [0.5700876116752625, 0.9993413090705872, 0.997...   neutral  \n",
       "11425  [0.9953122138977051, 0.9984742999076843, 0.989...   neutral  \n",
       "\n",
       "[11426 rows x 11 columns]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a0e80b1-5cb7-4e51-b776-baedf7cb500d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_article.to_pickle('art.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "28a77146-4cf1-408c-bad0-d23c0841de0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'entailment': 0.3, 'neutral': 99.7, 'contradiction': 0.1}\n",
      "{'entailment': 0.1, 'neutral': 99.6, 'contradiction': 0.3}\n",
      "{'entailment': 0.1, 'neutral': 99.6, 'contradiction': 0.3}\n",
      "{'entailment': 0.2, 'neutral': 1.7, 'contradiction': 98.1}\n",
      "{'entailment': 0.2, 'neutral': 99.2, 'contradiction': 0.6}\n"
     ]
    }
   ],
   "source": [
    "# tokenizer = AutoTokenizer.from_pretrained(nli_model)\n",
    "# model = AutoModelForSequenceClassification.from_pretrained(nli_model)\n",
    "\n",
    "# for sentence in top_5[0]:\n",
    "#     premise = row['sentence']\n",
    "#     hypothesis = df_article[df_article['company']==company]['sentence'].values[sentence['corpus_id']]\n",
    "#     tokens = tokenizer(premise, hypothesis, truncation=True, return_tensors=\"pt\")\n",
    "#     output = model(tokens[\"input_ids\"].to(device))  # device = \"cuda:0\" or \"cpu\"\n",
    "#     prediction = torch.softmax(output[\"logits\"][0], -1).tolist()\n",
    "#     label_names = [\"entailment\", \"neutral\", \"contradiction\"]\n",
    "#     prediction = {name: round(float(pred) * 100, 1) for pred, name in zip(prediction, label_names)}\n",
    "#     print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03776886-9e61-4b47-bcba-85e2677ef4e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # in case the input sentence is too long:\n",
    "# input_id_chunks = tokens_plus['input_ids'][0].split(510)\n",
    "# mask_chunks = tokens_plus['attention_mask'][0].split(510)\n",
    "\n",
    "# input_id_chunks = list(input_id_chunks)\n",
    "# mask_chunks = list(mask_chunks)\n",
    "\n",
    "\n",
    "# chunksize = 512\n",
    "# for i in range(len(input_id_chunks)):\n",
    "#     input_id_chunks[i] = torch.cat([\n",
    "#         torch.Tensor([101]), input_id_chunks[i], torch.Tensor([102])\n",
    "#     ])\n",
    "#     mask_chunks[i] = torch.cat([\n",
    "#         torch.Tensor([1]), mask_chunks[i], torch.Tensor([1])\n",
    "#     ])\n",
    "#     pad_len = chunksize - input_id_chunks[i].shape[0]\n",
    "    \n",
    "#     if pad_len > 0:\n",
    "#         input_id_chunks[i] = torch.cat([\n",
    "#             input_id_chunks[i], torch.Tensor([0]*pad_len)\n",
    "#         ])\n",
    "#         mask_id_chunks[i] = torch.cat([\n",
    "#             mask_id_chunks[i], torch.Tensor([0]*pad_len)\n",
    "#         ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71a129ad-0984-401c-8645-472ea2d7a5bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_ids = torch.stack(input_id_chunks)\n",
    "# attention_mask = torch.stack(mask_chunks)\n",
    "\n",
    "# input_dict = {\n",
    "#     'input_ids':input_ids.long(),\n",
    "#     'attention_mask': attention_mask.int()\n",
    "# }\n",
    "# input_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6609a51-4e76-45ad-acb5-7abd6b34ba8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# outputs = model(**input_dict)\n",
    "\n",
    "# probs = torch.nn.functional.softmax(outputs[0], dim = -1)\n",
    "# probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0489ae33-d833-4a7e-9063-180735017ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean = probs.mean(dim = 0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
