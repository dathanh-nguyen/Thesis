{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8c31209f-18f4-481e-977e-5d9d7c13b352",
   "metadata": {},
   "source": [
    "This notebook calculates the potential greenwashing score by comparing the text sentiment between the CSR reports and the news articles of a company."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb74356a-3ad6-47d8-b07f-016678130249",
   "metadata": {},
   "source": [
    "# Set-up and loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed1a0760-6129-4c7b-acc1-7ca6d8ff3b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline, AutoTokenizer, AutoModel, AutoModelForSequenceClassification\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer, util, models\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba769ac-f9f1-40f2-8589-3cd758f23b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_data = '..\\\\data_structured'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b72083-f9cf-4e87-9065-079b4f99d531",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_comb = pd.read_pickle(os.path.join(path_data, 'comb.pkl'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee74d226-00e6-45ba-b765-adf78b851c85",
   "metadata": {},
   "source": [
    "# Sentiment Analysis (ClimateBERT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9686e07-7b47-42ae-9b16-3829666f18de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# unifying the sentiment probabilities and labels into one score\n",
    "def calculate_score(row):\n",
    "    if (row['sentiment'] == 'negative')|(row['sentiment'] == 'risk'):\n",
    "        return row['sentiment_probability']*(-1)\n",
    "    elif row['sentiment'] == 'neutral':\n",
    "        return row['sentiment_probability']*0\n",
    "    else:\n",
    "        return row['sentiment_probability']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7c2e303-c96b-4fc5-9f8b-98723253580d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a function to get sentiment scores and labels\n",
    "def get_sentiment_scores(df, model):\n",
    "    pipe_sentiment = pipeline(model = model, device = 0, batch_size = 64)\n",
    "    sentences = df['sentence'].tolist()  # Convert the column to a list\n",
    "\n",
    "    results = pipe_sentiment(sentences)\n",
    "    df['sentiment'] = [result['label'] for result in results]\n",
    "    df['sentiment_probability'] = [result['score'] for result in results]\n",
    "    df['sentiment_score'] = df.apply(calculate_score, axis = 1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e80e18e-c151-4ef3-9b8c-f153fc9d0759",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_climate = df_comb.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80558f9e-14ec-4ea9-a420-ef1fc54c85bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_climate = get_sentiment_scores(df_climate, 'climatebert/distilroberta-base-climate-sentiment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "784c6b54-d884-4288-a7ee-0b35005257e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_climate['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac6b667e-18ce-4d32-8223-9f0869219e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is to get the finbert scores for the comparison in the discussion section\n",
    "# %%time\n",
    "# df_comb = get_sentiment_scores(df_comb, 'ProsusAI/finbert')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0842e671-f94e-4956-801c-b87b8cd3f475",
   "metadata": {},
   "outputs": [],
   "source": [
    "sust_topics = [0,1,2,3,4,5,6,7,8,10,11,20,25]\n",
    "# this was a test to see whether excluding certain topics would change results significantly (specifically greenwashing accusations)\n",
    "#lim_topics = [0,1,2,3,4,5,6,7,8,10,11,20]\n",
    "# df_analyze = df_comb[df_comb['topics'].isin(sust_topics)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b6d31a-e1ea-4002-8cb5-ef926ab40556",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_analyze = df_climate[df_climate['topics'].isin(sust_topics)]\n",
    "df_analyze.reset_index(inplace = True, drop = True)\n",
    "df_analyze.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39cb044c-15b2-4722-9751-21979a759817",
   "metadata": {},
   "source": [
    "## Firm-Level Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf164ab8-514a-42ff-aab6-9002170b1b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculating the average sentiment discrepancy without clusters\n",
    "gw_database_total = {}\n",
    "for firm in set(df_analyze['company']):\n",
    "     # calculate the average sentiment sc|ore for this firm for symbolic actions\n",
    "    rep_score = df_analyze[(df_analyze['company']==firm)&(df_analyze['doc_type']=='report')]['sentiment_score'].mean(skipna=True)\n",
    "    # calculate the average sentiment score for this firm for substantive actions\n",
    "    news_score = df_analyze[(df_analyze['company']==firm)&(df_analyze['doc_type']=='news')]['sentiment_score'].mean(skipna=True)\n",
    "    gw_database_total[firm] = (rep_score-news_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fe92366-a93d-495c-b8db-5368b4ae83e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the dataframe with the firm level scores\n",
    "df_gw_total = pd.DataFrame(gw_database_total.items(),columns=['company', 'clim_sentiment_overall'])\n",
    "df_gw_total"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efc63ac1-648e-429c-a02d-c95d8b0de636",
   "metadata": {},
   "source": [
    "## Cluster-Level Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ba400a-25f6-477b-ae1a-6f9de49bd253",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gw_score_sent_average(firm, cluster, df, mode= None):\n",
    "    # calculate the average sentiment score for this firm in this cluster across its report\n",
    "    rep_score = df[(df['company']==firm)&(df['topics']==cluster)&(df['doc_type']=='report')]['sentiment_score'].mean(skipna=True)\n",
    "    # calculate the average sentiment score for this firm in this cluster across its news coverage\n",
    "    news_score = df[(df['company']==firm)&(df['topics']==cluster)&(df['doc_type']=='news')]['sentiment_score'].mean(skipna=True)\n",
    "    if mode == 'weighted':\n",
    "        # for calculating the cluster importance as described by Boelders, we need to divide the number of cluster sentences for this company by its total number of sentences\n",
    "        cluster_sentences = len(df[(df['company']==firm)&(df['topics']==cluster)])\n",
    "        total_sentences = len(df[df['company']==firm])\n",
    "        cl_importance = cluster_sentences/total_sentences\n",
    "        return (rep_score-news_score)*cl_importance\n",
    "    else:\n",
    "        return (rep_score-news_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2463ebd9-5917-4da7-a93b-d7dfd99a35df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculating with clusters\n",
    "gw_database = {}\n",
    "for cluster in list(set(df_analyze['topics'])):\n",
    "    firm_cluster_score = {}\n",
    "    for firm in set(df_analyze['company']):\n",
    "        firm_cluster_score[firm] = gw_score_sent_average(firm, cluster, df_analyze)\n",
    "    gw_database[cluster]= firm_cluster_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65297a6c-68aa-4a06-be32-71cb8c43f6b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataframe with topic level scores\n",
    "df_gw = pd.DataFrame.from_dict(gw_database)\n",
    "df_gw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18639bad-0b04-465d-a678-0b24d4968239",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gw.reset_index(inplace = True)\n",
    "df_gw.rename(columns = {'index':'company'}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b5a7bf-e950-416b-bd54-5baef0db9c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculating the average across topics\n",
    "df_gw['clim_sentiment_cluster_average'] = df_gw[sust_topics].mean(axis = 1)\n",
    "df_gw"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82e50342-483f-4671-a27b-27be65a03050",
   "metadata": {},
   "source": [
    "Finally, I merge the firm-level and cluster-level scores into one table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c34b7f-05fa-4d4e-aad4-e60d1c932090",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merging the two datasets together and exporting it\n",
    "merged_df = df_gw_total.copy()\n",
    "merged_df = merged_df.merge(df_gw[['company','clim_sentiment_cluster_average']])\n",
    "# merged_df.to_csv('sentiment_scores.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
